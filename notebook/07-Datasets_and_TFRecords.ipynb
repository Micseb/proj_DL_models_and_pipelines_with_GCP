{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST classification: how to feed the data to the model\n",
    "1- MNIST dataset in memory  \n",
    "2- Transform the numpy array as images  \n",
    "3- Create and save the JPEG images on the Cloud Storage  \n",
    "4- Dataset to read numpy array in memory  \n",
    "5- Create TFRecords to store numpy array  \n",
    "6- Dataset to read TFRecord with numpy array  \n",
    "7- Create TFRecords to store JPEG images  \n",
    "8- Dataset to read TFRecord with JPEG images  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Install packages on Google  Cloud Datalab (locally use conda env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Select in the Python3 Kernel:\n",
    "In the menu bar the of 'Kernel', select   \n",
    "**python3**\n",
    "### Install needed packages\n",
    "copy the command below in a Google Cloud Datalab cell  \n",
    "**!pip install tensorflow==1.12**\n",
    "### Restart the Kernel \n",
    "this is to take into account the new installed packages. Click in the menu bar on:  \n",
    "**Reset Session**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Include paths to our functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tarrade/Desktop/Work/Data_Science/Tutorials_Codes/Python/proj_DL_models_and_pipelines_with_GCP/notebook\n",
      "/Users/tarrade/Desktop/Work/Data_Science/Tutorials_Codes/Python/proj_DL_models_and_pipelines_with_GCP\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "workingdir=os.getcwd()\n",
    "print(workingdir)\n",
    "d=[d for d in os.listdir(workingdir)]\n",
    "n=0\n",
    "while not set(['notebook']).issubset(set(d)):\n",
    "   workingdir=str(pathlib.Path(workingdir).parents[0])\n",
    "   print(workingdir)\n",
    "   d=[d for d in os.listdir(str(workingdir))]\n",
    "   n+=1\n",
    "   if n>5:\n",
    "       break\n",
    "sys.path.insert(0, workingdir)\n",
    "os.chdir(workingdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup librairies import and plots style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import gzip\n",
    "import sys\n",
    "import _pickle as cPickle\n",
    "from PIL import Image\n",
    "import glob as glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n",
      "2.1.6-tf\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import our utils functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.utils.mnist_utils as mnist_utils\n",
    "import src.utils.tensorflow_helper as tensorflow_helper\n",
    "import src.model_mnist_v1.trainer.model as mnist_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(mnist_utils)\n",
    "importlib.reload(mnist_v1)\n",
    "importlib.reload(tensorflow_helper);# to reload the function and mask the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Data\n",
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data: path is relative to the python path!\n",
    "(x_train, y_train), (x_test, y_test) = mnist_utils.load_data(path='data/mnist/raw/mnist.pkl.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check data shape (training)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check data shape (train)\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('uint8'), dtype('uint8'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.dtype, x_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(255, 0, 255, 0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(x_train), np.min(x_train), np.max(x_test), np.min(x_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Numpy array as JPEG as images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_images='data/mnist/images_train/'\n",
    "path_test_images='data/mnist/images_test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(path_train_images):\n",
    "    os.makedirs(path_train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(path_test_images):\n",
    "    os.makedirs(path_test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, im_array in enumerate(x_train):\n",
    "    im = Image.fromarray(im_array)\n",
    "    im.save(path_train_images+'image_train_'+str(i).zfill(5)+'_label_'+str(y_train[i]).zfill(2)+'.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, im_array in enumerate(x_test):\n",
    "    im = Image.fromarray(im_array)\n",
    "    im.save(path_test_images+'image_test_'+str(i).zfill(5)+'_label_'+str(y_test[i]).zfill(2)+'.jpeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save JPEG images on GCP"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!gcloud info --format='value(config.project)'\n",
    "!gcloud info --format='value(config.properties.compute.region)'\n",
    "!gcloud info --format='value(config.properties.compute.zone)'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!gcloud info --format=json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCS_BUCKET = 'gs://ml-productive-pipeline-53122' \n",
    "PROJECT = 'ml-productive-pipeline-53122'\n",
    "REGION = 'europe-west1'\n",
    "LOCAL_DATA_TEST = path_test_images\n",
    "LOCAL_DATA_TRAIN = path_train_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GCS_BUCKET'] = GCS_BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['LOCAL_DATA_TEST'] = LOCAL_DATA_TEST\n",
    "os.environ['LOCAL_DATA_TRAIN'] = LOCAL_DATA_TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://ml-productive-pipeline-53122/mnist/raw/test/\r\n",
      "gs://ml-productive-pipeline-53122/mnist/raw/train/\r\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls $GCS_BUCKET/mnist/raw"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!gsutil -m cp -R $LOCAL_DATA_TRAIN/ $GCS_BUCKET/mnist/image/train"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!gsutil -m cp -R $LOCAL_DATA_TEST/ $GCS_BUCKET/mnist/image/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of classes\n",
    "NUM_CLASSES =10\n",
    "\n",
    "# dimension of the input data\n",
    "DIM_INPUT = 784\n",
    "\n",
    "# number of epoch to train our model\n",
    "EPOCHS = 2\n",
    "\n",
    "# size of our mini batch\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# shuffle buffer size\n",
    "SHUFFLE_BUFFER_SIZE = 10 * BATCH_SIZE\n",
    "\n",
    "# prefetch buffer size\n",
    "PREFETCH_BUFFER_SIZE = tf.contrib.data.AUTOTUNE\n",
    "\n",
    "# number of paralell calls\n",
    "NUM_PARALELL_CALL = 4\n",
    "\n",
    "# model version\n",
    "MODEL='v1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defined flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorflow_helper.del_all_flags(tf.flags.FLAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just for jupyter notebook and avoir : \"UnrecognizedFlagError: Unknown command line flag 'f'\"\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel') \n",
    "\n",
    "# path to store the model and input for Tensorboard\n",
    "tf.app.flags.DEFINE_string('model_dir_keras', './results/Models/Mnist/tf_1_12/keras/'+MODEL+'/ckpt/', 'Dir to save a model and checkpoints with keras')\n",
    "tf.app.flags.DEFINE_string('tensorboard_dir_keras', './results/Models/Mnist/tf_1_12/keras/'+MODEL+'/logs/', 'Dir to save logs for TensorBoard with keras')\n",
    "\n",
    "# parameters for the input dataset and train the model\n",
    "tf.app.flags.DEFINE_integer('epoch', EPOCHS, 'number of epoch')\n",
    "tf.app.flags.DEFINE_integer('step_per_epoch', len(x_train) // BATCH_SIZE, 'number of step per epoch')\n",
    "tf.app.flags.DEFINE_integer('batch_size', BATCH_SIZE, 'Batch size')\n",
    "tf.app.flags.DEFINE_integer('shuffle_buffer_size', SHUFFLE_BUFFER_SIZE , 'Shuffle buffer size')\n",
    "tf.app.flags.DEFINE_integer('prefetch_buffer_size', PREFETCH_BUFFER_SIZE, 'Prefetch buffer size')\n",
    "tf.app.flags.DEFINE_integer('num_parallel_calls', NUM_PARALELL_CALL, 'Number of paralell calls')\n",
    "\n",
    "# parameters for the model\n",
    "tf.app.flags.DEFINE_integer('num_classes', NUM_CLASSES, 'number of classes in our model')\n",
    "tf.app.flags.DEFINE_integer('dim_input', DIM_INPUT, 'dimension of the input data for our model')\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset to preprocess and feed data in our model \n",
    "Use tf.data.dataset it will prepare mini batches of data, reshuffle the data, parallelized the pre-processing the data. \n",
    "\n",
    "https://www.tensorflow.org/guide/performance/datasets  \n",
    "To summarize, one good order for the different transformations is:\n",
    "- create the dataset\n",
    "- shuffle (with a big enough buffer size)  \n",
    "https://stackoverflow.com/questions/46444018/meaning-of-buffer-size-in-dataset-map-dataset-prefetch-and-dataset-shuffle)\n",
    "- repeat\n",
    "- map with the actual work (preprocessing, augmentation…) using multiple parallel calls\n",
    "- batch\n",
    "- prefetch\n",
    "\n",
    "ModeKeys:  \n",
    "https://www.tensorflow.org/api_docs/python/tf/estimator/ModeKeys  \n",
    "- EVAL\n",
    "- PREDICT\n",
    "- TRAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the number relater to the number of events (epoch, batch size, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary_input(data, step='training'):\n",
    "    print('Summary for the {} dataset:'.format(step))\n",
    "    if step=='training':\n",
    "        print('  - number of epoch            :', FLAGS.epoch)\n",
    "        print('  - number of events per epoch :', len(data))\n",
    "        print('  - batch size                 :', FLAGS.batch_size)\n",
    "        print('  - number of step per epoch   :', FLAGS.step_per_epoch)\n",
    "        print('  - total number of steps      :', FLAGS.epoch * FLAGS.step_per_epoch)\n",
    "    else:\n",
    "        print('  - number of epoch            :', 1)\n",
    "        print('  - number of events per epoch :', len(data))\n",
    "        print('  - batch size                 :', None)\n",
    "        print('  - number of step per epoch   :', 1)\n",
    "        print('  - total number of steps      :', 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for the training dataset:\n",
      "  - number of epoch            : 2\n",
      "  - number of events per epoch : 60000\n",
      "  - batch size                 : 128\n",
      "  - number of step per epoch   : 468\n",
      "  - total number of steps      : 936\n"
     ]
    }
   ],
   "source": [
    "print_summary_input(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for the training dataset:\n",
      "  - number of epoch            : 2\n",
      "  - number of events per epoch : 10000\n",
      "  - batch size                 : 128\n",
      "  - number of step per epoch   : 468\n",
      "  - total number of steps      : 936\n"
     ]
    }
   ],
   "source": [
    "print_summary_input(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset using numpy array to preprocess and feed data in our model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating \"Graph\" for the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_dataset_fn: TRAIN, train\n"
     ]
    }
   ],
   "source": [
    "training_dataset = mnist_v1.input_mnist_array_dataset_fn(x_train, \n",
    "                                                         y_train, \n",
    "                                                         FLAGS,\n",
    "                                                         mode=tf.estimator.ModeKeys.TRAIN, \n",
    "                                                         batch_size=FLAGS.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_dataset_fn: EVAL, eval\n"
     ]
    }
   ],
   "source": [
    "testing_dataset = mnist_v1.input_mnist_array_dataset_fn(x_test, \n",
    "                                                        y_test,\n",
    "                                                        FLAGS,\n",
    "                                                        mode=tf.estimator.ModeKeys.EVAL, \n",
    "                                                        batch_size=len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PrefetchDataset shapes: ((128, 784), (128, 10)), types: (tf.float32, tf.float32)>,\n",
       " <PrefetchDataset shapes: ((10000, 784), (10000, 10)), types: (tf.float32, tf.float32)>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset, testing_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Executing the \"Graph for the datasets\" for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an iterator\n",
    "iterator = training_dataset.make_one_shot_iterator()\n",
    "\n",
    "# next_element\n",
    "features, labels = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration n: 0 execution time: 2.2003929999999983 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 9 \n",
      "\n",
      "iteration n: 1 execution time: 0.02623299999999773 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 2 \n",
      "\n",
      "iteration n: 2 execution time: 0.02943599999999691 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 6 \n",
      "\n",
      "iteration n: 3 execution time: 0.029065000000002783 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 1 \n",
      "\n",
      "iteration n: 4 execution time: 0.030915000000000248 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 5 \n",
      "\n",
      "iteration n: 5 execution time: 0.029243999999998493 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 1 \n",
      "\n",
      "iteration n: 6 execution time: 0.029104000000003794 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 9 \n",
      "\n",
      "iteration n: 7 execution time: 0.03149700000000166 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 8 \n",
      "\n",
      "iteration n: 8 execution time: 0.028980000000004225 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 2 \n",
      "\n",
      "iteration n: 9 execution time: 0.028424000000001115 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 3 \n",
      "\n",
      "iteration n: 10 execution time: 0.030904000000006704 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 4 \n",
      "\n",
      "iteration n: 11 execution time: 0.026976000000004774 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 2 \n",
      "\n",
      "number of iteration reached\n"
     ]
    }
   ],
   "source": [
    "n=0\n",
    "\n",
    "n_iter=12\n",
    "with tf.Session() as sess:\n",
    "    while True:\n",
    "        try:\n",
    "            start_time = time.clock()\n",
    "            x,y = sess.run([features, labels])\n",
    "            print('iteration n:', n, 'execution time:', time.clock() - start_time, 'seconds')\n",
    "            print(x.shape)\n",
    "            print(y.shape)\n",
    "            print('first label of the batch',np.argmax(y[0]),'\\n')\n",
    "            n+=1\n",
    "            if n>=n_iter:\n",
    "                print('number of iteration reached')\n",
    "                break\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('tf.errors.OutOfRangeError')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Executing the \"Graph for the datasets\" for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an iterator\n",
    "iterator = testing_dataset.make_one_shot_iterator()\n",
    "\n",
    "# next_element\n",
    "features, labels = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration n: 0 execution time: 2.9108459999999994 seconds\n",
      "(10000, 784)\n",
      "(10000, 10)\n",
      "first label of the batch 7 \n",
      "\n",
      "iteration n: 1 execution time: 2.260881999999995 seconds\n",
      "(10000, 784)\n",
      "(10000, 10)\n",
      "first label of the batch 7 \n",
      "\n",
      "tf.errors.OutOfRangeError\n"
     ]
    }
   ],
   "source": [
    "n=0\n",
    "\n",
    "n_iter=10\n",
    "with tf.Session() as sess:\n",
    "    while True:\n",
    "        try:\n",
    "            start_time = time.clock()\n",
    "            x,y = sess.run([features, labels])\n",
    "            print('iteration n:', n, 'execution time:', time.clock() - start_time, 'seconds')\n",
    "            print(x.shape)\n",
    "            print(y.shape)\n",
    "            print('first label of the batch',np.argmax(y[0]),'\\n')\n",
    "            n+=1\n",
    "            if n>=n_iter:\n",
    "                print('number of iteration reached')\n",
    "                break\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('tf.errors.OutOfRangeError')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stored the input data as TFRecords files\n",
    "TFRecord file format is a simple record-oriented binary format  \n",
    "- https://medium.com/coinmonks/storage-efficient-tfrecord-for-images-6dc322b81db4\n",
    "- https://www.damienpontifex.com/2017/09/18/convert-and-using-the-mnist-dataset-as-tfrecords/\n",
    "- https://docs.databricks.com/_static/notebooks/horovodrunner/mnist-tensorflow-to-tfrecords.html\n",
    "\n",
    "Contrary to numpy array or pandas dataframe this is will scale with any amount of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy array to TFRecords files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the TFRecords files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test_tfrecords = 'data/mnist/tfrecord_numpy_test'\n",
    "path_train_tfrecords = 'data/mnist/tfrecord_numpy_train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /Users/tarrade/Desktop/Work/Data_Science/Tutorials_Codes/Python/proj_DL_models_and_pipelines_with_GCP/data/mnist/tfrecord_numpy_train/train.tfrecords data\n",
      "Processing sample 59131 of 60000"
     ]
    }
   ],
   "source": [
    "# creating TFRecords files for the training dataset\n",
    "mnist_v1.convert_numpy_to_tfrecords(x_train, y_train, 'train', path_train_tfrecords, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sample 9811 of 10000"
     ]
    }
   ],
   "source": [
    "# creating TFRecords files for the testing dataset\n",
    "mnist_v1.convert_numpy_to_tfrecords(x_test, y_test, 'test', path_test_tfrecords, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save TFRecord file with numpy array on GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_DATA_TEST = path_test_tfrecords\n",
    "LOCAL_DATA_TRAIN = path_train_tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['LOCAL_DATA_TEST'] = LOCAL_DATA_TEST\n",
    "os.environ['LOCAL_DATA_TRAIN'] = LOCAL_DATA_TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://ml-productive-pipeline-53122/mnist/image/\r\n",
      "gs://ml-productive-pipeline-53122/mnist/raw/\r\n",
      "gs://ml-productive-pipeline-53122/mnist/tfrecords/\r\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls $GCS_BUCKET/mnist/"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!gsutil -m cp -R $LOCAL_DATA_TRAIN/ $GCS_BUCKET/mnist/tfrecords/numpy_train"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!gsutil -m cp -R $LOCAL_DATA_TEST/ $GCS_BUCKET/mnist/tfrecords/numpy_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset using TFRecords file to preprocess and feed data in our model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_dataset_fn: TRAIN, train\n"
     ]
    }
   ],
   "source": [
    "training_dataset = mnist_v1.input_mnist_tfrecord_dataset_fn(glob.glob(path_train_tfrecords+'/train*.tfrecords'), \n",
    "                                                            FLAGS,\n",
    "                                                            mode=tf.estimator.ModeKeys.TRAIN, \n",
    "                                                            batch_size=FLAGS.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_dataset_fn: EVAL, eval\n"
     ]
    }
   ],
   "source": [
    "testing_dataset = mnist_v1.input_mnist_tfrecord_dataset_fn(glob.glob(path_test_tfrecords+'/test*.tfrecords'),\n",
    "                                                           FLAGS,\n",
    "                                                           mode=tf.estimator.ModeKeys.EVAL, \n",
    "                                                           batch_size=len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = training_dataset.make_one_shot_iterator()\n",
    "# next_element\n",
    "features, labels = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration n: 0 execution time: 0.8850599999999531 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 7 \n",
      "\n",
      "iteration n: 1 execution time: 0.06717600000001767 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 6 \n",
      "\n",
      "iteration n: 2 execution time: 0.06733999999994467 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 0 \n",
      "\n",
      "iteration n: 3 execution time: 0.05961200000001554 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 0 \n",
      "\n",
      "iteration n: 4 execution time: 0.06687399999998433 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 3 \n",
      "\n",
      "iteration n: 5 execution time: 0.06518599999992603 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 1 \n",
      "\n",
      "iteration n: 6 execution time: 0.058546999999975924 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 9 \n",
      "\n",
      "iteration n: 7 execution time: 0.05697999999995318 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 0 \n",
      "\n",
      "iteration n: 8 execution time: 0.05748299999993378 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 9 \n",
      "\n",
      "iteration n: 9 execution time: 0.049415000000067266 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 9 \n",
      "\n",
      "number of iteration reached\n"
     ]
    }
   ],
   "source": [
    "n=0\n",
    "n_iter=10\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    while True:\n",
    "        try:\n",
    "            start_time = time.clock()\n",
    "            x,y = sess.run([features, labels])\n",
    "            print('iteration n:', n, 'execution time:', time.clock() - start_time, 'seconds')\n",
    "            print(x.shape)\n",
    "            print(y.shape)\n",
    "            print('first label of the batch',np.argmax(y[0]),'\\n')\n",
    "            n+=1\n",
    "            if n>=n_iter:\n",
    "                print('number of iteration reached')\n",
    "                break\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('tf.errors.OutOfRangeError')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = testing_dataset.make_one_shot_iterator()\n",
    "# next_element\n",
    "features, labels = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration n: 0 execution time: 3.697167000000036 seconds\n",
      "(10000, 784)\n",
      "(10000, 10)\n",
      "first label of the batch 4 \n",
      "\n",
      "iteration n: 1 execution time: 3.2143539999999575 seconds\n",
      "(10000, 784)\n",
      "(10000, 10)\n",
      "first label of the batch 4 \n",
      "\n",
      "tf.errors.OutOfRangeError\n"
     ]
    }
   ],
   "source": [
    "n=0\n",
    "\n",
    "n_iter=10\n",
    "with tf.Session() as sess:\n",
    "    while True:\n",
    "        try:\n",
    "            start_time = time.clock()\n",
    "            x,y = sess.run([features, labels])\n",
    "            print('iteration n:', n, 'execution time:', time.clock() - start_time, 'seconds')\n",
    "            print(x.shape)\n",
    "            print(y.shape)\n",
    "            print('first label of the batch',np.argmax(y[0]),'\\n')\n",
    "            n+=1\n",
    "            if n>=n_iter:\n",
    "                print('number of iteration reached')\n",
    "                break\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('tf.errors.OutOfRangeError')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JPEG images to TFRecords files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the TFRecords files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test_image_tfrecords = 'data/mnist/tfrecord_image_test'\n",
    "path_train_image_tfrecords = 'data/mnist/tfrecord_image_train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /Users/tarrade/Desktop/Work/Data_Science/Tutorials_Codes/Python/proj_DL_models_and_pipelines_with_GCP/data/mnist/tfrecord_image_train/train-1.tfrecords data\n",
      "Processing sample 6000 of 6000\n",
      "Processing /Users/tarrade/Desktop/Work/Data_Science/Tutorials_Codes/Python/proj_DL_models_and_pipelines_with_GCP/data/mnist/tfrecord_image_train/train-2.tfrecords data\n",
      "Processing sample 6000 of 6000\n",
      "Processing /Users/tarrade/Desktop/Work/Data_Science/Tutorials_Codes/Python/proj_DL_models_and_pipelines_with_GCP/data/mnist/tfrecord_image_train/train-3.tfrecords data\n",
      "Processing sample 6000 of 6000\n",
      "Processing /Users/tarrade/Desktop/Work/Data_Science/Tutorials_Codes/Python/proj_DL_models_and_pipelines_with_GCP/data/mnist/tfrecord_image_train/train-4.tfrecords data\n",
      "Processing sample 6000 of 6000\n",
      "Processing /Users/tarrade/Desktop/Work/Data_Science/Tutorials_Codes/Python/proj_DL_models_and_pipelines_with_GCP/data/mnist/tfrecord_image_train/train-5.tfrecords data\n",
      "Processing sample 6000 of 6000\n",
      "Processing /Users/tarrade/Desktop/Work/Data_Science/Tutorials_Codes/Python/proj_DL_models_and_pipelines_with_GCP/data/mnist/tfrecord_image_train/train-6.tfrecords data\n",
      "Processing sample 6000 of 6000\n",
      "Processing /Users/tarrade/Desktop/Work/Data_Science/Tutorials_Codes/Python/proj_DL_models_and_pipelines_with_GCP/data/mnist/tfrecord_image_train/train-7.tfrecords data\n",
      "Processing sample 6000 of 6000\n",
      "Processing /Users/tarrade/Desktop/Work/Data_Science/Tutorials_Codes/Python/proj_DL_models_and_pipelines_with_GCP/data/mnist/tfrecord_image_train/train-8.tfrecords data\n",
      "Processing sample 6000 of 6000\n",
      "Processing /Users/tarrade/Desktop/Work/Data_Science/Tutorials_Codes/Python/proj_DL_models_and_pipelines_with_GCP/data/mnist/tfrecord_image_train/train-9.tfrecords data\n",
      "Processing sample 6000 of 6000\n",
      "Processing /Users/tarrade/Desktop/Work/Data_Science/Tutorials_Codes/Python/proj_DL_models_and_pipelines_with_GCP/data/mnist/tfrecord_image_train/train-10.tfrecords data\n",
      "Processing sample 6000 of 6000\n"
     ]
    }
   ],
   "source": [
    "# creating TFRecords files for the training dataset\n",
    "mnist_v1.convert_image_to_tfrecords(glob.glob(path_train_images+'/*.jpeg'), 'train', path_train_tfrecords, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /Users/tarrade/Desktop/Work/Data_Science/Tutorials_Codes/Python/proj_DL_models_and_pipelines_with_GCP/data/mnist/tfrecord_image_test/test-1.tfrecords data\n",
      "Processing sample 1000 of 1000\n",
      "Processing /Users/tarrade/Desktop/Work/Data_Science/Tutorials_Codes/Python/proj_DL_models_and_pipelines_with_GCP/data/mnist/tfrecord_image_test/test-2.tfrecords data\n",
      "Processing sample 1000 of 1000\n",
      "Processing /Users/tarrade/Desktop/Work/Data_Science/Tutorials_Codes/Python/proj_DL_models_and_pipelines_with_GCP/data/mnist/tfrecord_image_test/test-3.tfrecords data\n",
      "Processing sample 1000 of 1000\n",
      "Processing /Users/tarrade/Desktop/Work/Data_Science/Tutorials_Codes/Python/proj_DL_models_and_pipelines_with_GCP/data/mnist/tfrecord_image_test/test-4.tfrecords data\n",
      "Processing sample 1000 of 1000\n",
      "Processing /Users/tarrade/Desktop/Work/Data_Science/Tutorials_Codes/Python/proj_DL_models_and_pipelines_with_GCP/data/mnist/tfrecord_image_test/test-5.tfrecords data\n",
      "Processing sample 1000 of 1000\n",
      "Processing /Users/tarrade/Desktop/Work/Data_Science/Tutorials_Codes/Python/proj_DL_models_and_pipelines_with_GCP/data/mnist/tfrecord_image_test/test-6.tfrecords data\n",
      "Processing sample 1000 of 1000\n",
      "Processing /Users/tarrade/Desktop/Work/Data_Science/Tutorials_Codes/Python/proj_DL_models_and_pipelines_with_GCP/data/mnist/tfrecord_image_test/test-7.tfrecords data\n",
      "Processing sample 1000 of 1000\n",
      "Processing /Users/tarrade/Desktop/Work/Data_Science/Tutorials_Codes/Python/proj_DL_models_and_pipelines_with_GCP/data/mnist/tfrecord_image_test/test-8.tfrecords data\n",
      "Processing sample 1000 of 1000\n",
      "Processing /Users/tarrade/Desktop/Work/Data_Science/Tutorials_Codes/Python/proj_DL_models_and_pipelines_with_GCP/data/mnist/tfrecord_image_test/test-9.tfrecords data\n",
      "Processing sample 1000 of 1000\n",
      "Processing /Users/tarrade/Desktop/Work/Data_Science/Tutorials_Codes/Python/proj_DL_models_and_pipelines_with_GCP/data/mnist/tfrecord_image_test/test-10.tfrecords data\n",
      "Processing sample 1000 of 1000\n"
     ]
    }
   ],
   "source": [
    "# creating TFRecords files for the testing dataset\n",
    "mnist_v1.convert_image_to_tfrecords(glob.glob(path_test_images+'/*.jpeg'), 'test', path_test_tfrecords, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save TFRecord file with JPEG image on GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_DATA_TEST = path_test_image_tfrecords\n",
    "LOCAL_DATA_TRAIN = path_train_image_tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['LOCAL_DATA_TEST'] = LOCAL_DATA_TEST\n",
    "os.environ['LOCAL_DATA_TRAIN'] = LOCAL_DATA_TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://ml-productive-pipeline-53122/mnist/image/\r\n",
      "gs://ml-productive-pipeline-53122/mnist/raw/\r\n",
      "gs://ml-productive-pipeline-53122/mnist/tfrecords/\r\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls $GCS_BUCKET/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://data/mnist/tfrecord_image_train/train-10.tfrecords [Content-Type=application/octet-stream]...\n",
      "Copying file://data/mnist/tfrecord_image_train/train-3.tfrecords [Content-Type=application/octet-stream]...\n",
      "Copying file://data/mnist/tfrecord_image_train/train-4.tfrecords [Content-Type=application/octet-stream]...\n",
      "Copying file://data/mnist/tfrecord_image_train/train-2.tfrecords [Content-Type=application/octet-stream]...\n",
      "Copying file://data/mnist/tfrecord_image_train/train-9.tfrecords [Content-Type=application/octet-stream]...\n",
      "Copying file://data/mnist/tfrecord_image_train/train-8.tfrecords [Content-Type=application/octet-stream]...\n",
      "Copying file://data/mnist/tfrecord_image_train/train-6.tfrecords [Content-Type=application/octet-stream]...\n",
      "Copying file://data/mnist/tfrecord_image_train/.DS_Store [Content-Type=application/octet-stream]...\n",
      "Copying file://data/mnist/tfrecord_image_train/train-1.tfrecords [Content-Type=application/octet-stream]...\n",
      "Copying file://data/mnist/tfrecord_image_train/train-7.tfrecords [Content-Type=application/octet-stream]...\n",
      "Copying file://data/mnist/tfrecord_image_train/train-5.tfrecords [Content-Type=application/octet-stream]...\n",
      "- [11/11 files][ 48.2 MiB/ 48.2 MiB] 100% Done   3.1 MiB/s ETA 00:00:00         \n",
      "Operation completed over 11 objects/48.2 MiB.                                    \n"
     ]
    }
   ],
   "source": [
    "!gsutil -m cp -R $LOCAL_DATA_TRAIN/ $GCS_BUCKET/mnist/tfrecords/image_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://data/mnist/tfrecord_image_test/test-3.tfrecords [Content-Type=application/octet-stream]...\n",
      "Copying file://data/mnist/tfrecord_image_test/test-7.tfrecords [Content-Type=application/octet-stream]...\n",
      "Copying file://data/mnist/tfrecord_image_test/test-10.tfrecords [Content-Type=application/octet-stream]...\n",
      "Copying file://data/mnist/tfrecord_image_test/test-4.tfrecords [Content-Type=application/octet-stream]...\n",
      "Copying file://data/mnist/tfrecord_image_test/test-2.tfrecords [Content-Type=application/octet-stream]...\n",
      "Copying file://data/mnist/tfrecord_image_test/test-8.tfrecords [Content-Type=application/octet-stream]...\n",
      "Copying file://data/mnist/tfrecord_image_test/test-5.tfrecords [Content-Type=application/octet-stream]...\n",
      "Copying file://data/mnist/tfrecord_image_test/test-1.tfrecords [Content-Type=application/octet-stream]...\n",
      "Copying file://data/mnist/tfrecord_image_test/test-9.tfrecords [Content-Type=application/octet-stream]...\n",
      "Copying file://data/mnist/tfrecord_image_test/test-6.tfrecords [Content-Type=application/octet-stream]...\n",
      "- [10/10 files][  8.0 MiB/  8.0 MiB] 100% Done                                  \n",
      "Operation completed over 10 objects/8.0 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil -m cp -R $LOCAL_DATA_TEST/ $GCS_BUCKET/mnist/tfrecords/image_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset using TFRecords file to preprocess and feed data in our model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_dataset_fn: TRAIN, train\n"
     ]
    }
   ],
   "source": [
    "training_dataset = mnist_v1.input_mnist_tfrecord_dataset_fn(glob.glob(path_train_image_tfrecords+'/train*.tfrecords'), \n",
    "                                                            FLAGS,\n",
    "                                                            mode=tf.estimator.ModeKeys.TRAIN, \n",
    "                                                            batch_size=FLAGS.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_dataset_fn: EVAL, eval\n"
     ]
    }
   ],
   "source": [
    "testing_dataset = mnist_v1.input_mnist_tfrecord_dataset_fn(glob.glob(path_test_image_tfrecords+'/test*.tfrecords'),\n",
    "                                                           FLAGS,\n",
    "                                                           mode=tf.estimator.ModeKeys.EVAL, \n",
    "                                                           batch_size=len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = training_dataset.make_one_shot_iterator()\n",
    "# next_element\n",
    "features, labels = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration n: 0 execution time: 0.7266619999999193 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 7 \n",
      "\n",
      "iteration n: 1 execution time: 0.059810000000084074 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 6 \n",
      "\n",
      "iteration n: 2 execution time: 0.05490500000007614 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 0 \n",
      "\n",
      "iteration n: 3 execution time: 0.05208000000004631 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 0 \n",
      "\n",
      "iteration n: 4 execution time: 0.04858000000001539 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 3 \n",
      "\n",
      "iteration n: 5 execution time: 0.049823999999944135 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 1 \n",
      "\n",
      "iteration n: 6 execution time: 0.04981699999996181 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 9 \n",
      "\n",
      "iteration n: 7 execution time: 0.05104200000005221 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 0 \n",
      "\n",
      "iteration n: 8 execution time: 0.049216999999998734 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 9 \n",
      "\n",
      "iteration n: 9 execution time: 0.043626000000017484 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 9 \n",
      "\n",
      "number of iteration reached\n"
     ]
    }
   ],
   "source": [
    "n=0\n",
    "n_iter=10\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    while True:\n",
    "        try:\n",
    "            start_time = time.clock()\n",
    "            x,y = sess.run([features, labels])\n",
    "            print('iteration n:', n, 'execution time:', time.clock() - start_time, 'seconds')\n",
    "            print(x.shape)\n",
    "            print(y.shape)\n",
    "            print('first label of the batch',np.argmax(y[0]),'\\n')\n",
    "            n+=1\n",
    "            if n>=n_iter:\n",
    "                print('number of iteration reached')\n",
    "                break\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('tf.errors.OutOfRangeError')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = testing_dataset.make_one_shot_iterator()\n",
    "# next_element\n",
    "features, labels = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration n: 0 execution time: 3.788214000000039 seconds\n",
      "(10000, 784)\n",
      "(10000, 10)\n",
      "first label of the batch 4 \n",
      "\n",
      "iteration n: 1 execution time: 3.382412000000045 seconds\n",
      "(10000, 784)\n",
      "(10000, 10)\n",
      "first label of the batch 4 \n",
      "\n",
      "tf.errors.OutOfRangeError\n"
     ]
    }
   ],
   "source": [
    "n=0\n",
    "\n",
    "n_iter=10\n",
    "with tf.Session() as sess:\n",
    "    while True:\n",
    "        try:\n",
    "            start_time = time.clock()\n",
    "            x,y = sess.run([features, labels])\n",
    "            print('iteration n:', n, 'execution time:', time.clock() - start_time, 'seconds')\n",
    "            print(x.shape)\n",
    "            print(y.shape)\n",
    "            print('first label of the batch',np.argmax(y[0]),'\\n')\n",
    "            n+=1\n",
    "            if n>=n_iter:\n",
    "                print('number of iteration reached')\n",
    "                break\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('tf.errors.OutOfRangeError')\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_gcp_dl]",
   "language": "python",
   "name": "conda-env-env_gcp_dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

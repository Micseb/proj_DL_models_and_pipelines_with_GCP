{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST classification: how to feed the data to the model\n",
    "1- MNIST dataset in memory  \n",
    "2- Transform the numpy array as images  \n",
    "3- Create and save the JPEG images on the Cloud Storage  \n",
    "4- Dataset to read numpy array in memory  \n",
    "5- Create TFRecords to store numpy array  \n",
    "6- Dataset to read TFRecord with numpy array  \n",
    "7- Create TFRecords to store JPEG images  \n",
    "8- Dataset to read TFRecord with JPEG images  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Install packages on Google  Cloud Datalab (locally use conda env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Select in the Python3 Kernel:\n",
    "In the menu bar the of 'Kernel', select   \n",
    "**python3**\n",
    "### Install needed packages\n",
    "copy the command below in a Google Cloud Datalab cell  \n",
    "**!pip install tensorflow==1.12**\n",
    "### Restart the Kernel \n",
    "this is to take into account the new installed packages. Click in the menu bar on:  \n",
    "**Reset Session**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Include paths to our functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tarrade/Desktop/Work/Data_Science/Tutorials_Codes/Python/proj_DL_models_and_pipelines_with_GCP/notebook\n",
      "/Users/tarrade/Desktop/Work/Data_Science/Tutorials_Codes/Python/proj_DL_models_and_pipelines_with_GCP\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "workingdir=os.getcwd()\n",
    "print(workingdir)\n",
    "d=[d for d in os.listdir(workingdir)]\n",
    "n=0\n",
    "while not set(['notebook']).issubset(set(d)):\n",
    "   workingdir=str(pathlib.Path(workingdir).parents[0])\n",
    "   print(workingdir)\n",
    "   d=[d for d in os.listdir(str(workingdir))]\n",
    "   n+=1\n",
    "   if n>5:\n",
    "       break\n",
    "sys.path.insert(0, workingdir)\n",
    "os.chdir(workingdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup librairies import and plots style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import gzip\n",
    "import sys\n",
    "import _pickle as cPickle\n",
    "from PIL import Image\n",
    "import glob as glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n",
      "2.1.6-tf\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import our utils functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.utils.mnist_utils as mnist_utils\n",
    "import src.utils.tensorflow_helper as tensorflow_helper\n",
    "import src.model_mnist_v1.trainer.model as mnist_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(mnist_utils)\n",
    "importlib.reload(mnist_v1)\n",
    "importlib.reload(tensorflow_helper);# to reload the function and mask the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Data\n",
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data: path is relative to the python path!\n",
    "(x_train, y_train), (x_test, y_test) = mnist_utils.load_data(path='data/mnist/raw/mnist.pkl.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check data shape (training)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check data shape (train)\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('uint8'), dtype('uint8'))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.dtype, x_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(255, 0, 255, 0)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(x_train), np.min(x_train), np.max(x_test), np.min(x_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Numpy array as JPEG as images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_images='data/mnist/images_train/'\n",
    "path_test_images='data/mnist/images_test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(path_train_images):\n",
    "    os.makedirs(path_train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(path_test_images):\n",
    "    os.makedirs(path_test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, im_array in enumerate(x_train):\n",
    "    im = Image.fromarray(im_array)\n",
    "    im.save(path_train_images+'image_train_'+str(i).zfill(5)+'_label_'+str(y_train[i]).zfill(2)+'.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, im_array in enumerate(x_test):\n",
    "    im = Image.fromarray(im_array)\n",
    "    im.save(path_test_images+'image_test_'+str(i).zfill(5)+'_label_'+str(y_test[i]).zfill(2)+'.jpeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save JPEG images on GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml-productive-pipeline-53122\n",
      "europe-west1\n",
      "europe-west1-c\n"
     ]
    }
   ],
   "source": [
    "!gcloud info --format='value(config.project)'\n",
    "!gcloud info --format='value(config.properties.compute.region)'\n",
    "!gcloud info --format='value(config.properties.compute.zone)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"basic\": {\r\n",
      "    \"architecture\": {\r\n",
      "      \"file_name\": \"x86_64\",\r\n",
      "      \"id\": \"x86_64\",\r\n",
      "      \"name\": \"x86_64\"\r\n",
      "    },\r\n",
      "    \"operating_system\": {\r\n",
      "      \"file_name\": \"darwin\",\r\n",
      "      \"id\": \"MACOSX\",\r\n",
      "      \"name\": \"Mac OS X\"\r\n",
      "    },\r\n",
      "    \"python_location\": \"/Users/tarrade/anaconda/bin/python2\",\r\n",
      "    \"python_version\": \"2.7.15 |Anaconda, Inc.| (default, Nov 13 2018, 17:07:45) \\n[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\",\r\n",
      "    \"site_packages\": false,\r\n",
      "    \"version\": \"234.0.0\"\r\n",
      "  },\r\n",
      "  \"config\": {\r\n",
      "    \"account\": \"fabien.tarrade@gmail.com\",\r\n",
      "    \"active_config_name\": \"default\",\r\n",
      "    \"paths\": {\r\n",
      "      \"active_config_path\": \"/Users/tarrade/.config/gcloud/configurations/config_default\",\r\n",
      "      \"global_config_dir\": \"/Users/tarrade/.config/gcloud\",\r\n",
      "      \"installation_properties_path\": \"/Users/tarrade/Test/google-cloud-sdk/properties\"\r\n",
      "    },\r\n",
      "    \"project\": \"ml-productive-pipeline-53122\",\r\n",
      "    \"properties\": {\r\n",
      "      \"compute\": {\r\n",
      "        \"region\": \"europe-west1\",\r\n",
      "        \"zone\": \"europe-west1-c\"\r\n",
      "      },\r\n",
      "      \"core\": {\r\n",
      "        \"account\": \"fabien.tarrade@gmail.com\",\r\n",
      "        \"disable_usage_reporting\": \"False\",\r\n",
      "        \"project\": \"ml-productive-pipeline-53122\"\r\n",
      "      }\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"env_proxy\": {\r\n",
      "    \"address\": null,\r\n",
      "    \"password\": null,\r\n",
      "    \"port\": null,\r\n",
      "    \"type\": null,\r\n",
      "    \"username\": null\r\n",
      "  },\r\n",
      "  \"installation\": {\r\n",
      "    \"additional_repos\": [],\r\n",
      "    \"components\": {\r\n",
      "      \"bq\": \"2.0.41\",\r\n",
      "      \"core\": \"2019.02.08\",\r\n",
      "      \"gsutil\": \"4.36\"\r\n",
      "    },\r\n",
      "    \"duplicate_tool_paths\": [],\r\n",
      "    \"kubectl\": \"/usr/local/bin/kubectl\",\r\n",
      "    \"old_tool_paths\": [],\r\n",
      "    \"on_path\": true,\r\n",
      "    \"path\": [\r\n",
      "      \"/Users/tarrade/anaconda3/envs/env_gcp_dl/bin\",\r\n",
      "      \"/Users/tarrade/Test/google-cloud-sdk/bin\",\r\n",
      "      \"/Users/tarrade/anaconda3/bin\",\r\n",
      "      \"/Users/tarrade/anaconda3/condabin\",\r\n",
      "      \"/Users/tarrade/anaconda/bin\",\r\n",
      "      \"/Users/tarrade/anaconda/bin\",\r\n",
      "      \"/Users/tarrade/anaconda/bin\",\r\n",
      "      \"/Users/tarrade/anaconda/bin\",\r\n",
      "      \"/Users/tarrade/anaconda/bin\",\r\n",
      "      \"/Users/tarrade/miniconda2/bin\",\r\n",
      "      \"/opt/local/bin\",\r\n",
      "      \"/opt/local/sbin\",\r\n",
      "      \"/opt/local/bin\",\r\n",
      "      \"/opt/local/sbin\",\r\n",
      "      \"/usr/local/bin\",\r\n",
      "      \"/usr/local/bin\",\r\n",
      "      \"/usr/bin\",\r\n",
      "      \"/bin\",\r\n",
      "      \"/usr/sbin\",\r\n",
      "      \"/sbin\",\r\n",
      "      \"/usr/texbin\",\r\n",
      "      \"/opt/X11/bin\"\r\n",
      "    ],\r\n",
      "    \"python_path\": [\r\n",
      "      \"/Users/tarrade/Test/google-cloud-sdk/lib/third_party\",\r\n",
      "      \"/Users/tarrade/Test/google-cloud-sdk/lib\",\r\n",
      "      \"/Users/tarrade/anaconda/lib/python27.zip\",\r\n",
      "      \"/Users/tarrade/anaconda/lib/python2.7\",\r\n",
      "      \"/Users/tarrade/anaconda/lib/python2.7/plat-darwin\",\r\n",
      "      \"/Users/tarrade/anaconda/lib/python2.7/plat-mac\",\r\n",
      "      \"/Users/tarrade/anaconda/lib/python2.7/plat-mac/lib-scriptpackages\",\r\n",
      "      \"/Users/tarrade/anaconda/lib/python2.7/lib-tk\",\r\n",
      "      \"/Users/tarrade/anaconda/lib/python2.7/lib-old\",\r\n",
      "      \"/Users/tarrade/anaconda/lib/python2.7/lib-dynload\"\r\n",
      "    ],\r\n",
      "    \"release_channel\": \"rapid\",\r\n",
      "    \"repo_url\": \"https://dl.google.com/dl/cloudsdk/channels/rapid/components-2.json\",\r\n",
      "    \"sdk_root\": \"/Users/tarrade/Test/google-cloud-sdk\"\r\n",
      "  },\r\n",
      "  \"logs\": {\r\n",
      "    \"last_log\": \"/Users/tarrade/.config/gcloud/logs/2019.02.27/14.11.21.874067.log\",\r\n",
      "    \"last_logs\": [\r\n",
      "      \"/Users/tarrade/.config/gcloud/logs/2019.02.27/14.11.21.874067.log\",\r\n",
      "      \"/Users/tarrade/.config/gcloud/logs/2019.02.27/14.11.16.867212.log\",\r\n",
      "      \"/Users/tarrade/.config/gcloud/logs/2019.02.27/14.11.07.717480.log\",\r\n",
      "      \"/Users/tarrade/.config/gcloud/logs/2019.02.27/14.08.40.015655.log\",\r\n",
      "      \"/Users/tarrade/.config/gcloud/logs/2019.02.27/14.08.37.000643.log\"\r\n",
      "    ],\r\n",
      "    \"logs_dir\": \"/Users/tarrade/.config/gcloud/logs\"\r\n",
      "  },\r\n",
      "  \"tools\": {\r\n",
      "    \"git_version\": \"git version 2.7.0\",\r\n",
      "    \"ssh_version\": \"OpenSSH_7.9p1, LibreSSL 2.7.3\"\r\n",
      "  }\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!gcloud info --format=json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCS_BUCKET = 'gs://ml-productive-pipeline-53122' \n",
    "PROJECT = 'ml-productive-pipeline-53122'\n",
    "REGION = 'europe-west1'\n",
    "LOCAL_DATA_TEST = path_test_images\n",
    "LOCAL_DATA_TRAIN = path_train_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GCS_BUCKET'] = GCS_BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['LOCAL_DATA_TEST'] = LOCAL_DATA_TEST\n",
    "os.environ['LOCAL_DATA_TRAIN'] = LOCAL_DATA_TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://ml-productive-pipeline-53122/mnist/raw/test/\r\n",
      "gs://ml-productive-pipeline-53122/mnist/raw/train/\r\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls $GCS_BUCKET/mnist/raw"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!gsutil -m cp -R $LOCAL_DATA_TRAIN/ $GCS_BUCKET/mnist/image/train"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!gsutil -m cp -R $LOCAL_DATA_TEST/ $GCS_BUCKET/mnist/image/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of classes\n",
    "NUM_CLASSES =10\n",
    "\n",
    "# dimension of the input data\n",
    "DIM_INPUT = 784\n",
    "\n",
    "# number of epoch to train our model\n",
    "EPOCHS = 2\n",
    "\n",
    "# size of our mini batch\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# shuffle buffer size\n",
    "SHUFFLE_BUFFER_SIZE = 10 * BATCH_SIZE\n",
    "\n",
    "# prefetch buffer size\n",
    "PREFETCH_BUFFER_SIZE = tf.contrib.data.AUTOTUNE\n",
    "\n",
    "# number of paralell calls\n",
    "NUM_PARALELL_CALL = 4\n",
    "\n",
    "# model version\n",
    "MODEL='v1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defined flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorflow_helper.del_all_flags(tf.flags.FLAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just for jupyter notebook and avoir : \"UnrecognizedFlagError: Unknown command line flag 'f'\"\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel') \n",
    "\n",
    "# path to store the model and input for Tensorboard\n",
    "tf.app.flags.DEFINE_string('model_dir_keras', './results/Models/Mnist/tf_1_12/keras/'+MODEL+'/ckpt/', 'Dir to save a model and checkpoints with keras')\n",
    "tf.app.flags.DEFINE_string('tensorboard_dir_keras', './results/Models/Mnist/tf_1_12/keras/'+MODEL+'/logs/', 'Dir to save logs for TensorBoard with keras')\n",
    "\n",
    "# parameters for the input dataset and train the model\n",
    "tf.app.flags.DEFINE_integer('epoch', EPOCHS, 'number of epoch')\n",
    "tf.app.flags.DEFINE_integer('step_per_epoch', len(x_train) // BATCH_SIZE, 'number of step per epoch')\n",
    "tf.app.flags.DEFINE_integer('batch_size', BATCH_SIZE, 'Batch size')\n",
    "tf.app.flags.DEFINE_integer('shuffle_buffer_size', SHUFFLE_BUFFER_SIZE , 'Shuffle buffer size')\n",
    "tf.app.flags.DEFINE_integer('prefetch_buffer_size', PREFETCH_BUFFER_SIZE, 'Prefetch buffer size')\n",
    "tf.app.flags.DEFINE_integer('num_parallel_calls', NUM_PARALELL_CALL, 'Number of paralell calls')\n",
    "\n",
    "# parameters for the model\n",
    "tf.app.flags.DEFINE_integer('num_classes', NUM_CLASSES, 'number of classes in our model')\n",
    "tf.app.flags.DEFINE_integer('dim_input', DIM_INPUT, 'dimension of the input data for our model')\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset to preprocess and feed data in our model \n",
    "Use tf.data.dataset it will prepare mini batches of data, reshuffle the data, parallelized the pre-processing the data. \n",
    "\n",
    "https://www.tensorflow.org/guide/performance/datasets  \n",
    "To summarize, one good order for the different transformations is:\n",
    "- create the dataset\n",
    "- shuffle (with a big enough buffer size)  \n",
    "https://stackoverflow.com/questions/46444018/meaning-of-buffer-size-in-dataset-map-dataset-prefetch-and-dataset-shuffle)\n",
    "- repeat\n",
    "- map with the actual work (preprocessing, augmentation…) using multiple parallel calls\n",
    "- batch\n",
    "- prefetch\n",
    "\n",
    "ModeKeys:  \n",
    "https://www.tensorflow.org/api_docs/python/tf/estimator/ModeKeys  \n",
    "- EVAL\n",
    "- PREDICT\n",
    "- TRAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the number relater to the number of events (epoch, batch size, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary_input(data, step='training'):\n",
    "    print('Summary for the {} dataset:'.format(step))\n",
    "    if step=='training':\n",
    "        print('  - number of epoch            :', FLAGS.epoch)\n",
    "        print('  - number of events per epoch :', len(data))\n",
    "        print('  - batch size                 :', FLAGS.batch_size)\n",
    "        print('  - number of step per epoch   :', FLAGS.step_per_epoch)\n",
    "        print('  - total number of steps      :', FLAGS.epoch * FLAGS.step_per_epoch)\n",
    "    else:\n",
    "        print('  - number of epoch            :', 1)\n",
    "        print('  - number of events per epoch :', len(data))\n",
    "        print('  - batch size                 :', None)\n",
    "        print('  - number of step per epoch   :', 1)\n",
    "        print('  - total number of steps      :', 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for the training dataset:\n",
      "  - number of epoch            : 10\n",
      "  - number of events per epoch : 60000\n",
      "  - batch size                 : 128\n",
      "  - number of step per epoch   : 468\n",
      "  - total number of steps      : 4680\n"
     ]
    }
   ],
   "source": [
    "print_summary_input(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for the training dataset:\n",
      "  - number of epoch            : 10\n",
      "  - number of events per epoch : 10000\n",
      "  - batch size                 : 128\n",
      "  - number of step per epoch   : 468\n",
      "  - total number of steps      : 4680\n"
     ]
    }
   ],
   "source": [
    "print_summary_input(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset using numpy array to preprocess and feed data in our model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating \"Graph\" for the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_dataset_fn: TRAIN, train\n"
     ]
    }
   ],
   "source": [
    "training_dataset = mnist_v1.input_mnist_array_dataset_fn(x_train, \n",
    "                                                         y_train, \n",
    "                                                         FLAGS,\n",
    "                                                         mode=tf.estimator.ModeKeys.TRAIN, \n",
    "                                                         batch_size=FLAGS.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_dataset_fn: EVAL, eval\n"
     ]
    }
   ],
   "source": [
    "testing_dataset = mnist_v1.input_mnist_array_dataset_fn(x_test, \n",
    "                                                        y_test,\n",
    "                                                        FLAGS,\n",
    "                                                        mode=tf.estimator.ModeKeys.EVAL, \n",
    "                                                        batch_size=len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PrefetchDataset shapes: ((128, 784), (128, 10)), types: (tf.float32, tf.float32)>,\n",
       " <PrefetchDataset shapes: ((10000, 784), (10000, 10)), types: (tf.float32, tf.float32)>)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset, testing_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Executing the \"Graph for the datasets\" for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an iterator\n",
    "iterator = training_dataset.make_one_shot_iterator()\n",
    "\n",
    "# next_element\n",
    "features, labels = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration n: 0 execution time: 2.063270000000017 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 9 \n",
      "\n",
      "iteration n: 1 execution time: 0.02947199999999839 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 2 \n",
      "\n",
      "iteration n: 2 execution time: 0.030369000000007418 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 6 \n",
      "\n",
      "iteration n: 3 execution time: 0.028968999999989364 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 1 \n",
      "\n",
      "iteration n: 4 execution time: 0.028436999999996715 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 5 \n",
      "\n",
      "iteration n: 5 execution time: 0.029730999999998176 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 1 \n",
      "\n",
      "iteration n: 6 execution time: 0.02815099999997983 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 9 \n",
      "\n",
      "iteration n: 7 execution time: 0.02917199999998843 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 8 \n",
      "\n",
      "iteration n: 8 execution time: 0.028329000000013593 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 2 \n",
      "\n",
      "iteration n: 9 execution time: 0.02890899999999874 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 3 \n",
      "\n",
      "iteration n: 10 execution time: 0.0282009999999957 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 4 \n",
      "\n",
      "iteration n: 11 execution time: 0.028773000000001048 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 2 \n",
      "\n",
      "number of iteration reached\n"
     ]
    }
   ],
   "source": [
    "n=0\n",
    "\n",
    "n_iter=12\n",
    "with tf.Session() as sess:\n",
    "    while True:\n",
    "        try:\n",
    "            start_time = time.clock()\n",
    "            x,y = sess.run([features, labels])\n",
    "            print('iteration n:', n, 'execution time:', time.clock() - start_time, 'seconds')\n",
    "            print(x.shape)\n",
    "            print(y.shape)\n",
    "            print('first label of the batch',np.argmax(y[0]),'\\n')\n",
    "            n+=1\n",
    "            if n>=n_iter:\n",
    "                print('number of iteration reached')\n",
    "                break\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('tf.errors.OutOfRangeError')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Executing the \"Graph for the datasets\" for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an iterator\n",
    "iterator = testing_dataset.make_one_shot_iterator()\n",
    "\n",
    "# next_element\n",
    "features, labels = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration n: 0 execution time: 2.8718900000000076 seconds\n",
      "(10000, 784)\n",
      "(10000, 10)\n",
      "first label of the batch 7 \n",
      "\n",
      "iteration n: 1 execution time: 2.057161000000008 seconds\n",
      "(10000, 784)\n",
      "(10000, 10)\n",
      "first label of the batch 7 \n",
      "\n",
      "iteration n: 2 execution time: 1.7940890000000138 seconds\n",
      "(10000, 784)\n",
      "(10000, 10)\n",
      "first label of the batch 7 \n",
      "\n",
      "iteration n: 3 execution time: 2.1304879999999855 seconds\n",
      "(10000, 784)\n",
      "(10000, 10)\n",
      "first label of the batch 7 \n",
      "\n",
      "iteration n: 4 execution time: 2.154911999999996 seconds\n",
      "(10000, 784)\n",
      "(10000, 10)\n",
      "first label of the batch 7 \n",
      "\n",
      "iteration n: 5 execution time: 2.1948069999999973 seconds\n",
      "(10000, 784)\n",
      "(10000, 10)\n",
      "first label of the batch 7 \n",
      "\n",
      "iteration n: 6 execution time: 2.1568949999999916 seconds\n",
      "(10000, 784)\n",
      "(10000, 10)\n",
      "first label of the batch 7 \n",
      "\n",
      "iteration n: 7 execution time: 2.387498999999991 seconds\n",
      "(10000, 784)\n",
      "(10000, 10)\n",
      "first label of the batch 7 \n",
      "\n",
      "iteration n: 8 execution time: 1.8869400000000098 seconds\n",
      "(10000, 784)\n",
      "(10000, 10)\n",
      "first label of the batch 7 \n",
      "\n",
      "iteration n: 9 execution time: 1.8634899999999845 seconds\n",
      "(10000, 784)\n",
      "(10000, 10)\n",
      "first label of the batch 7 \n",
      "\n",
      "number of iteration reached\n"
     ]
    }
   ],
   "source": [
    "n=0\n",
    "\n",
    "n_iter=10\n",
    "with tf.Session() as sess:\n",
    "    while True:\n",
    "        try:\n",
    "            start_time = time.clock()\n",
    "            x,y = sess.run([features, labels])\n",
    "            print('iteration n:', n, 'execution time:', time.clock() - start_time, 'seconds')\n",
    "            print(x.shape)\n",
    "            print(y.shape)\n",
    "            print('first label of the batch',np.argmax(y[0]),'\\n')\n",
    "            n+=1\n",
    "            if n>=n_iter:\n",
    "                print('number of iteration reached')\n",
    "                break\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('tf.errors.OutOfRangeError')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stored the input data as TFRecords files\n",
    "TFRecord file format is a simple record-oriented binary format  \n",
    "- https://medium.com/coinmonks/storage-efficient-tfrecord-for-images-6dc322b81db4\n",
    "- https://www.damienpontifex.com/2017/09/18/convert-and-using-the-mnist-dataset-as-tfrecords/\n",
    "- https://docs.databricks.com/_static/notebooks/horovodrunner/mnist-tensorflow-to-tfrecords.html\n",
    "\n",
    "Contrary to numpy array or pandas dataframe this is will scale with any amount of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function for TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be move in .py\n",
    "def _int64_feature(value:int) -> tf.train.Features.FeatureEntry:\n",
    "    \"\"\"Create a Int64List Feature\n",
    "    \n",
    "    Args:\n",
    "        value: The value to store in the feature\n",
    "    \n",
    "    Returns:\n",
    "        The FeatureEntry\n",
    "    \"\"\"\n",
    "    \n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be deleted ?\n",
    "def _floatlist_feature(value:str) -> tf.train.Features.FeatureEntry:\n",
    "    \"\"\"Create a FloatList Feature\n",
    "    \n",
    "    Args:\n",
    "        value: The value to store in the feature\n",
    "    \n",
    "    Returns:\n",
    "        The FeatureEntry\n",
    "    \"\"\"\n",
    "    \n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be move in .py\n",
    "def _bytes_feature(value:str) -> tf.train.Features.FeatureEntry:\n",
    "    \"\"\"Create a BytesList Feature\n",
    "    \n",
    "    Args:\n",
    "        value: The value to store in the feature\n",
    "    \n",
    "    Returns:\n",
    "        The FeatureEntry\n",
    "    \"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _data_path(data_directory:str, name:str) -> str:\n",
    "    \"\"\"Construct a full path to a TFRecord file to be stored in the \n",
    "    data_directory. Will also ensure the data directory exists\n",
    "    \n",
    "    Args:\n",
    "        data_directory: The directory where the records will be stored\n",
    "        name:           The name of the TFRecord\n",
    "    \n",
    "    Returns:\n",
    "        The full path to the TFRecord file\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(data_directory):\n",
    "        os.makedirs(data_directory)\n",
    "\n",
    "    return os.path.join(data_directory, f'{name}.tfrecords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy array to TFRecords files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the TFRecords files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _numpy_to_tfrecords(example_dataset, filename:str):\n",
    "    print(f'Processing {filename} data')\n",
    "    dataset_length = len(example_dataset)\n",
    "    with tf.python_io.TFRecordWriter(filename) as writer:\n",
    "        for index, (image, label) in enumerate(example_dataset):\n",
    "            sys.stdout.write(f\"\\rProcessing sample {index+1} of {dataset_length}\")\n",
    "            sys.stdout.flush()\n",
    "            image_raw = image.tostring()\n",
    "            example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                'label': _int64_feature(int(label)),\n",
    "                'image_raw': _bytes_feature(image_raw)\n",
    "            }))            \n",
    "            writer.write(example.SerializeToString())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_numpy_to_tfrecords(x_data, y_data, name:str, data_directory:str, num_shards:int=1):\n",
    "    \"\"\"Convert the dataset into TFRecords on disk\n",
    "    \n",
    "    Args:\n",
    "        x_data:         The MNIST data set to convert: data\n",
    "        y_data:         The MNIST data set to convert: label\n",
    "        name:           The name of the data set\n",
    "        data_directory: The directory where records will be stored\n",
    "        num_shards:     The number of files on disk to separate records into\n",
    "    \"\"\"\n",
    "\n",
    "    data_set = list(zip(x_data, y_data))\n",
    "    data_directory=os.path.abspath(data_directory)\n",
    "    \n",
    "    if num_shards == 1:\n",
    "        _numpy_to_tfrecords(data_set, _data_path(data_directory, name))\n",
    "    else:\n",
    "        sharded_dataset = np.array_split(data_set, num_shards)\n",
    "        for shard, dataset in enumerate(sharded_dataset):\n",
    "            _numpy_to_tfrecords(dataset, _data_path(data_directory, f'{name}-{shard+1}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test_tfrecords = 'data/mnist/tfrecord_numpy_test'\n",
    "path_train_tfrecords = 'data/mnist/tfrecord_numpy_train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /Users/tarrade/Desktop/Work/Data_Science/Tutorials_Codes/Python/proj_DL_models_and_pipelines_with_GCP/data/mnist/tfrecord_numpy_train/train.tfrecords data\n",
      "Processing sample 60000 of 60000\n"
     ]
    }
   ],
   "source": [
    "# creating TFRecords files for the training dataset\n",
    "convert_numpy_to_tfrecords(x_train, y_train, 'train', path_train_tfrecords, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /Users/tarrade/Desktop/Work/Data_Science/Tutorials_Codes/Python/proj_DL_models_and_pipelines_with_GCP/data/mnist/tfrecord_numpy_test/test.tfrecords data\n",
      "Processing sample 9774 of 10000"
     ]
    }
   ],
   "source": [
    "# creating TFRecords files for the testing dataset\n",
    "convert_numpy_to_tfrecords(x_test, y_test, 'test', path_test_tfrecords, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save TFRecord file with numpy array on GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_DATA_TEST = path_test_tfrecords\n",
    "LOCAL_DATA_TRAIN = path_train_tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['LOCAL_DATA_TEST'] = LOCAL_DATA_TEST\n",
    "os.environ['LOCAL_DATA_TRAIN'] = LOCAL_DATA_TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://ml-productive-pipeline-53122/mnist/image/\r\n",
      "gs://ml-productive-pipeline-53122/mnist/raw/\r\n",
      "gs://ml-productive-pipeline-53122/mnist/tfrecords/\r\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls $GCS_BUCKET/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://data/mnist/tfrecord_numpy_train/train.tfrecords [Content-Type=application/octet-stream]...\n",
      "- [1/1 files][ 48.2 MiB/ 48.2 MiB] 100% Done                                    \n",
      "Operation completed over 1 objects/48.2 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil -m cp -R $LOCAL_DATA_TRAIN/ $GCS_BUCKET/mnist/tfrecords/numpy_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://data/mnist/tfrecord_numpy_test/.DS_Store [Content-Type=application/octet-stream]...\n",
      "Copying file://data/mnist/tfrecord_numpy_test/test.tfrecords [Content-Type=application/octet-stream]...\n",
      "| [2/2 files][  8.0 MiB/  8.0 MiB] 100% Done                                    \n",
      "Operation completed over 2 objects/8.0 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil -m cp -R $LOCAL_DATA_TEST/ $GCS_BUCKET/mnist/tfrecords/numpy_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset using TFRecords file to preprocess and feed data in our model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_numpy_tfrecords_fn(filenames, batch_size=128, mode=tf.estimator.ModeKeys.TRAIN):\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        tf.logging.info(\"input_dataset_fn: PREDICT, {}\".format(mode))\n",
    "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "        tf.logging.info(\"input_dataset_fn: EVAL, {}\".format(mode))\n",
    "    elif mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        tf.logging.info(\"input_dataset_fn: TRAIN, {}\".format(mode))\n",
    "    \n",
    "    def _parser(record):\n",
    "        # 1. define a parser\n",
    "        features={\n",
    "            # the label are parsed as int\n",
    "            'label': tf.FixedLenFeature(shape=[], dtype=tf.int64),\n",
    "            # the bytes_list data is parsed into tf.string.\n",
    "            'image_raw': tf.FixedLenFeature(shape=[], dtype=tf.string)\n",
    "        }\n",
    "        print('step 1')\n",
    "        parsed_record = tf.parse_single_example(record, features)\n",
    "        print('step 2')\n",
    "        print(parsed_record)\n",
    "        # 2. Convert the data\n",
    "        label = parsed_record['label']\n",
    "        #image = parsed_record['image_raw']#\n",
    "        image =  tf.cast(tf.decode_raw(parsed_record['image_raw'], out_type=tf.uint8), tf.float64)\n",
    "        print('TEST',label.shape)\n",
    "        print('TEST',image.shape)\n",
    "        # 3. reshape\n",
    "        #tf.reshape(image, [1,784])\n",
    "        #print('---',image.shape())\n",
    "        \n",
    "        # 4. hot emcoding\n",
    "        num_classes=10\n",
    "        #label = tf.keras.utils.to_categorical( label, num_classes)\n",
    "        label=tf.one_hot(label, num_classes)\n",
    "\n",
    "        return image, label\n",
    "        #dataset = tf.data.Dataset.from_tensor_slices((image, label))\n",
    "        #return dataset\n",
    "    \n",
    "    def _normalize(image, label):\n",
    "        \"\"\"Convert image from [0, 255] -> [-0.5, 0.5] floats.\"\"\"\n",
    "        image = image * (1. / 255) - 0.5\n",
    "        return image, label\n",
    "    \n",
    "    def _dense_to_one_hot(labels_dense, num_classes):\n",
    "        \"\"\"Convert class labels from scalars to one-hot vectors.\"\"\"\n",
    "        num_labels = labels_dense.shape[0]\n",
    "        index_offset = numpy.arange(num_labels) * num_classes\n",
    "        labels_one_hot = numpy.zeros((num_labels, num_classes))\n",
    "        labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "        return labels_one_hot\n",
    "    \n",
    "    def _input_fn():\n",
    "        # 1) read data from TFRecordDataset\n",
    "        dataset = (tf.data.TFRecordDataset(filenames).map(_parser))\n",
    "        \n",
    "        # 2) shuffle (with a big enough buffer size)    :        \n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_epochs = None # loop indefinitely\n",
    "            dataset = dataset.shuffle(buffer_size=FLAGS.shuffle_buffer_size, seed=2)# depends on sample size\n",
    "        else:\n",
    "            num_epochs = 1 # end-of-input after this\n",
    "        print('the number of epoch: num_epoch =', num_epochs)\n",
    "        \n",
    "        # caching data\n",
    "        #dataset = dataset.cache()\n",
    "    \n",
    "        # 3) automatically refill the data queue when empty\n",
    "        dataset = dataset.repeat(num_epochs)\n",
    "\n",
    "        # 4) map\n",
    "        dataset = dataset.map(map_func=_normalize, num_parallel_calls=FLAGS.num_parallel_calls)\n",
    "\n",
    "        # 5) create batches of data\n",
    "        dataset = dataset.batch(batch_size=batch_size, drop_remainder=True)\n",
    "\n",
    "        # 6) prefetch data for faster consumption, based on your system and environment, allows the tf.data runtime to automatically tune the prefetch buffer sizes\n",
    "        dataset = dataset.prefetch(FLAGS.prefetch_buffer_size)\n",
    "\n",
    "        return dataset\n",
    "    return _input_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/mnist/tfrecord_numpy_train/train.tfrecords']"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob(path_train_tfrecords+'/train*.tfrecords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_dataset_fn: TRAIN, train\n",
      "step 1\n",
      "step 2\n",
      "{'image_raw': <tf.Tensor 'ParseSingleExample/ParseSingleExample:0' shape=() dtype=string>, 'label': <tf.Tensor 'ParseSingleExample/ParseSingleExample:1' shape=() dtype=int64>}\n",
      "TEST ()\n",
      "TEST (?,)\n",
      "the number of epoch: num_epoch = None\n"
     ]
    }
   ],
   "source": [
    "training_dataset = input_numpy_tfrecords_fn(glob.glob(path_train_tfrecords+'/train*.tfrecords'), \n",
    "                                            mode=tf.estimator.ModeKeys.TRAIN, \n",
    "                                            batch_size=FLAGS.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_dataset_fn: EVAL, eval\n",
      "step 1\n",
      "step 2\n",
      "{'image_raw': <tf.Tensor 'ParseSingleExample/ParseSingleExample:0' shape=() dtype=string>, 'label': <tf.Tensor 'ParseSingleExample/ParseSingleExample:1' shape=() dtype=int64>}\n",
      "TEST ()\n",
      "TEST (?,)\n",
      "the number of epoch: num_epoch = 1\n"
     ]
    }
   ],
   "source": [
    "testing_dataset = input_numpy_tfrecords_fn(glob.glob(path_test_tfrecords+'/test*.tfrecords'), \n",
    "                                           mode=tf.estimator.ModeKeys.EVAL, \n",
    "                                           batch_size=len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = training_dataset.make_one_shot_iterator()\n",
    "# next_element\n",
    "features, labels = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration n: 0 execution time: 0.8165169999999762 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 9 \n",
      "\n",
      "iteration n: 1 execution time: 0.06866899999999987 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 2 \n",
      "\n",
      "iteration n: 2 execution time: 0.06222000000002481 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 6 \n",
      "\n",
      "iteration n: 3 execution time: 0.06054800000003979 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 1 \n",
      "\n",
      "iteration n: 4 execution time: 0.06055100000003222 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 5 \n",
      "\n",
      "iteration n: 5 execution time: 0.06222900000000209 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 1 \n",
      "\n",
      "iteration n: 6 execution time: 0.06071299999996427 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 9 \n",
      "\n",
      "iteration n: 7 execution time: 0.07545900000002348 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 8 \n",
      "\n",
      "iteration n: 8 execution time: 0.05505900000002839 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 2 \n",
      "\n",
      "iteration n: 9 execution time: 0.05801799999994728 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 3 \n",
      "\n",
      "number of iteration reached\n"
     ]
    }
   ],
   "source": [
    "n=0\n",
    "n_iter=10\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    while True:\n",
    "        try:\n",
    "            start_time = time.clock()\n",
    "            x,y = sess.run([features, labels])\n",
    "            print('iteration n:', n, 'execution time:', time.clock() - start_time, 'seconds')\n",
    "            print(x.shape)\n",
    "            print(y.shape)\n",
    "            print('first label of the batch',np.argmax(y[0]),'\\n')\n",
    "            n+=1\n",
    "            if n>=n_iter:\n",
    "                print('number of iteration reached')\n",
    "                break\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('tf.errors.OutOfRangeError')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = testing_dataset.make_one_shot_iterator()\n",
    "# next_element\n",
    "features, labels = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration n: 0 execution time: 4.225740000000087 seconds\n",
      "(10000, 784)\n",
      "(10000, 10)\n",
      "first label of the batch 7 \n",
      "\n",
      "tf.errors.OutOfRangeError\n"
     ]
    }
   ],
   "source": [
    "n=0\n",
    "\n",
    "n_iter=10\n",
    "with tf.Session() as sess:\n",
    "    while True:\n",
    "        try:\n",
    "            start_time = time.clock()\n",
    "            x,y = sess.run([features, labels])\n",
    "            print('iteration n:', n, 'execution time:', time.clock() - start_time, 'seconds')\n",
    "            print(x.shape)\n",
    "            print(y.shape)\n",
    "            print('first label of the batch',np.argmax(y[0]),'\\n')\n",
    "            n+=1\n",
    "            if n>=n_iter:\n",
    "                print('number of iteration reached')\n",
    "                break\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('tf.errors.OutOfRangeError')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFRecord files based on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CLOUDSDK_PYTHON']='/Users/tarrade/anaconda3/bin/python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Cloud SDK 232.0.0\n",
      "bq 2.0.40\n",
      "core 2019.01.27\n",
      "gsutil 4.35\n"
     ]
    }
   ],
   "source": [
    "!gcloud version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CLOUDSDK_PYTHON']='/Users/tarrade/anaconda/bin/python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://../data/test.tfrecords [Content-Type=application/octet-stream]...\n",
      "Copying file://../data/train.tfrecords [Content-Type=application/octet-stream]...\n",
      "\\ [2 files][ 56.2 MiB/ 56.2 MiB]    4.9 MiB/s                                   \n",
      "Operation completed over 2 objects/56.2 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp ../data/*tfrecords gs://ml-productive-pipeline-53122"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_dataset_tfrecords_fn(filenames, batch_size=128, mode=tf.estimator.ModeKeys.TRAIN):\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        tf.logging.info(\"input_dataset_fn: PREDICT, {}\".format(mode))\n",
    "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "        tf.logging.info(\"input_dataset_fn: EVAL, {}\".format(mode))\n",
    "    elif mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        tf.logging.info(\"input_dataset_fn: TRAIN, {}\".format(mode))\n",
    "    \n",
    "    def _parser(record):\n",
    "        # 1. define a parser\n",
    "        features={\n",
    "            #'label': tf.FixedLenFeature([], tf.int64),\n",
    "            #'image_raw': tf.FixedLenFeature([], tf.train.FloatList)#tf.string)\n",
    "            'label': tf.FixedLenFeature(shape=[], dtype=tf.int64),\n",
    "            # The bytes_list data is parsed into tf.string.\n",
    "            'image_raw': tf.FixedLenFeature(shape=[], dtype=tf.string)\n",
    "        }\n",
    "        print('step 1')\n",
    "        parsed_record = tf.parse_single_example(record, features)\n",
    "        print('step 2')\n",
    "        print(parsed_record)\n",
    "        # 2. Convert the data\n",
    "        label = parsed_record['label']\n",
    "        #image = parsed_record['image_raw']#\n",
    "        image =  tf.cast(tf.decode_raw(parsed_record['image_raw'], out_type=tf.uint8), tf.float64)\n",
    "        print('TEST',label.shape)\n",
    "        print('TEST',image.shape)\n",
    "        # 3. reshape\n",
    "        #tf.reshape(image, [1,784])\n",
    "        #print('---',image.shape())\n",
    "        \n",
    "        # 4. hot emcoding\n",
    "        num_classes=10\n",
    "        #label = tf.keras.utils.to_categorical( label, num_classes)\n",
    "        label=tf.one_hot(label, num_classes)\n",
    "\n",
    "        return image, label\n",
    "        #dataset = tf.data.Dataset.from_tensor_slices((image, label))\n",
    "        #return dataset\n",
    "    \n",
    "    def _normalize(image, label):\n",
    "        \"\"\"Convert image from [0, 255] -> [-0.5, 0.5] floats.\"\"\"\n",
    "        image = image * (1. / 255) - 0.5\n",
    "        return image, label\n",
    "    \n",
    "    def _dense_to_one_hot(labels_dense, num_classes):\n",
    "        \"\"\"Convert class labels from scalars to one-hot vectors.\"\"\"\n",
    "        num_labels = labels_dense.shape[0]\n",
    "        index_offset = numpy.arange(num_labels) * num_classes\n",
    "        labels_one_hot = numpy.zeros((num_labels, num_classes))\n",
    "        labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "        return labels_one_hot\n",
    "    \n",
    "    def _input_fn():\n",
    "        # 1) read data from TFRecordDataset\n",
    "        dataset = (tf.data.TFRecordDataset(filenames).map(_parser))\n",
    "        \n",
    "        # 2) shuffle (with a big enough buffer size)    :        \n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_epochs = None # loop indefinitely\n",
    "            dataset = dataset.shuffle(buffer_size=FLAGS.shuffle_buffer_size, seed=2)# depends on sample size\n",
    "        else:\n",
    "            num_epochs = 1 # end-of-input after this\n",
    "        print('the number of epoch: num_epoch =', num_epochs)\n",
    "        \n",
    "        # caching data\n",
    "        #dataset = dataset.cache()\n",
    "    \n",
    "        # 3) automatically refill the data queue when empty\n",
    "        dataset = dataset.repeat(num_epochs)\n",
    "\n",
    "        # 4) map\n",
    "        dataset = dataset.map(map_func=_normalize, num_parallel_calls=FLAGS.num_parallel_calls)\n",
    "\n",
    "        # 5) create batches of data\n",
    "        dataset = dataset.batch(batch_size=batch_size, drop_remainder=True)\n",
    "\n",
    "        # 6) prefetch data for faster consumption, based on your system and environment, allows the tf.data runtime to automatically tune the prefetch buffer sizes\n",
    "        dataset = dataset.prefetch(FLAGS.prefetch_buffer_size)\n",
    "\n",
    "        return dataset\n",
    "    return _input_fn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration dataset API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = training_dataset.make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next_element\n",
    "features, labels = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration n: 0 execution time: 0.5324680000001081 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 9 \n",
      "\n",
      "iteration n: 1 execution time: 0.055957000000034895 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 2 \n",
      "\n",
      "iteration n: 2 execution time: 0.05217799999991257 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 6 \n",
      "\n",
      "iteration n: 3 execution time: 0.04504799999995157 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 1 \n",
      "\n",
      "iteration n: 4 execution time: 0.04535500000019965 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 5 \n",
      "\n",
      "iteration n: 5 execution time: 0.04520900000011352 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 1 \n",
      "\n",
      "iteration n: 6 execution time: 0.04962000000000444 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 9 \n",
      "\n",
      "iteration n: 7 execution time: 0.050853999999844746 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 8 \n",
      "\n",
      "iteration n: 8 execution time: 0.04601600000000872 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 2 \n",
      "\n",
      "iteration n: 9 execution time: 0.046758999999838124 seconds\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "first label of the batch 3 \n",
      "\n",
      "number of iteration reached\n"
     ]
    }
   ],
   "source": [
    "n=0\n",
    "n_iter=10 #len(x_train)//BATCH_SIZE\n",
    "with tf.Session() as sess:\n",
    "    while True:\n",
    "        try:\n",
    "            start_time = time.clock()\n",
    "            x,y = sess.run([features, labels])\n",
    "            print('iteration n:', n, 'execution time:', time.clock() - start_time, 'seconds')\n",
    "            print(x.shape)\n",
    "            print(y.shape)\n",
    "            print('first label of the batch',np.argmax(y[0]),'\\n')\n",
    "            n+=1\n",
    "            if n>=n_iter:\n",
    "                print('number of iteration reached')\n",
    "                break\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('tf.errors.OutOfRangeError')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = testing_dataset.make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next_element\n",
    "features, labels = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6481220000000576 seconds\n",
      "(10000, 784)\n",
      "(10000, 10)\n",
      "first label of the batch 7 \n",
      "\n",
      "tf.errors.OutOfRangeError\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    while True:\n",
    "        try:\n",
    "            start_time = time.clock()\n",
    "            x,y = sess.run([features, labels])\n",
    "            print(time.clock() - start_time, 'seconds')\n",
    "            print(x.shape)\n",
    "            print(y.shape)\n",
    "            print('first label of the batch',np.argmax(y[0]),'\\n')\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('tf.errors.OutOfRangeError')\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_gcp_dl]",
   "language": "python",
   "name": "conda-env-env_gcp_dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

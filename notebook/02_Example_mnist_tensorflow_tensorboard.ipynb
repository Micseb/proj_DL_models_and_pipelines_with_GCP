{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST classification example with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Needed librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.datasets import mnist\n",
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mnist data, split between train and test sets\n",
    "# on GCP\n",
    "#(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# with AXA network\n",
    "import gzip\n",
    "import sys\n",
    "import _pickle as cPickle\n",
    "def load_data(path):\n",
    "    f = gzip.open(path, 'rb')\n",
    "    if sys.version_info < (3,):\n",
    "        data = cPickle.load(f)\n",
    "    else:\n",
    "        data = cPickle.load(f, encoding='bytes')\n",
    "    f.close()\n",
    "    return data\n",
    "(x_train, y_train), (x_test, y_test) = load_data(path='../data/mnist.pkl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data shape (training)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data shape (train)\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.dtype, x_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(x_train), np.min(x_train), np.max(x_test), np.min(x_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize and reorganize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast uint8 -> float32\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renormalize the data 255 grey variation\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the data 28 x 28 -> 784\n",
    "x_train = x_train.reshape(len(x_train), x_train.shape[1]*x_train.shape[2])\n",
    "x_test = x_test.reshape(len(x_test), x_test.shape[1]*x_test.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_train), np.unique(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(y_train))\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_input=x_train.shape[1]\n",
    "dim_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,4))\n",
    "for index, (image, label) in enumerate(zip(x_train[0:5], y_train[0:5])):\n",
    "    plt.subplot(1, 5, index + 1)\n",
    "    plt.imshow(np.reshape(image, (28,28)), cmap=plt.cm.gray)\n",
    "    plt.title('Training: %i\\n' % np.argmax(label), fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defined some hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate\n",
    "learning_rate = 0.5\n",
    "# number of epoch to train our model\n",
    "epochs = 10\n",
    "# size of our mini batch\n",
    "batch_size = 128\n",
    "# hidden layer 1\n",
    "n1=300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defined our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x [60000, 784]\n",
    "# y [60000, 10]\n",
    "\n",
    "# reset graph before starting\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# tensor (placeholder) for the learning rate\n",
    "learning_rate = tf.placeholder(tf.float32, shape=())\n",
    "\n",
    "# tensor (placeholder) for the input data [60000, 784]\n",
    "x = tf.placeholder(tf.float32, [None, dim_input])\n",
    "# tensor (placeholder) for the output data [60000, 10]\n",
    "y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "\n",
    "# now declare the weights connecting the input to the hidden layer\n",
    "W1 = tf.Variable(tf.random_normal([dim_input, n1], stddev=0.03), name='W1')\n",
    "b1 = tf.Variable(tf.random_normal([n1]), name='b1')\n",
    "# and the weights connecting the hidden layer to the output layer\n",
    "W2 = tf.Variable(tf.random_normal([n1, num_classes], stddev=0.03), name='W2')\n",
    "b2 = tf.Variable(tf.random_normal([num_classes]), name='b2')\n",
    "\n",
    "# calculate the output of the hidden layer\n",
    "hidden_out = tf.add(tf.matmul(x, W1), b1)\n",
    "hidden_out = tf.nn.relu(hidden_out)\n",
    "\n",
    "# now calculate the hidden layer output - in this case, let's use a softmax activated\n",
    "# output layer\n",
    "y_ = tf.nn.softmax(tf.add(tf.matmul(hidden_out, W2), b2))\n",
    "\n",
    "# now let's define the cost function which we are going to train the model on\n",
    "y_clipped = tf.clip_by_value(y_, 1e-10, 0.9999999)\n",
    "cross_entropy = -tf.reduce_mean(tf.reduce_sum(y * tf.log(y_clipped)\n",
    "                                                  + (1 - y) * tf.log(1 - y_clipped), axis=1))\n",
    "\n",
    "# add an optimiser\n",
    "optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "\n",
    "# finally setup the initialisation operator\n",
    "init= tf.global_variables_initializer()\n",
    "\n",
    "# define an accuracy assessment operation\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a summary to store the accuracy\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter('../results/TensorBoard/mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer = tf.summary.FileWriter('.')\n",
    "# writer.add_graph(tf.get_default_graph)\n",
    "# writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_logistic_model(learning_r, training_epochs, train_obs, train_labels, test_obs, test_labels, debug = False):\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    \n",
    "    writer = tf.summary.FileWriter(\"../results/TensorBoard/mnist\", sess.graph)\n",
    "    \n",
    "    cost_history = np.empty(shape=[0], dtype = float)\n",
    "\n",
    "    for epoch in range(training_epochs+1):\n",
    "        \n",
    "        sess.run(optimiser, feed_dict = {x: train_obs, y: train_labels, learning_rate: learning_r})\n",
    "\n",
    "        cost_ = sess.run(cross_entropy, feed_dict={x: train_obs, y: train_labels, learning_rate: learning_r})\n",
    "        cost_history = np.append(cost_history, cost_)\n",
    "        \n",
    "        if (epoch % 100 == 0) & debug:\n",
    "            print(\"Reached epoch\",epoch,\"cost J =\", str.format('{0:.6f}', cost_))\n",
    "            acc_train = sess.run(accuracy, feed_dict={x: train_obs, y: train_labels})\n",
    "            acc_test = sess.run(accuracy, feed_dict={x: test_obs, y: test_labels})\n",
    "            print(\" accurary on the training set\", str.format('{0:.2f}', acc_train))\n",
    "            print(\" accurary on the testing set\", str.format('{0:.2f}', acc_test))\n",
    "        summary = sess.run(merged, feed_dict={x: test_obs, y: test_labels})\n",
    "        writer.add_summary(summary, epoch)\n",
    "    print(\"\\nTraining complete!\")\n",
    "    writer.add_graph(sess.graph)\n",
    "    return sess, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess, cost_history = run_logistic_model(learning_r = 0.01, \n",
    "                                        training_epochs = 300, \n",
    "                                        train_obs = x_train, \n",
    "                                        train_labels = y_train, \n",
    "                                        test_obs = x_test, \n",
    "                                        test_labels = y_test, \n",
    "                                        debug = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', family='arial')\n",
    "plt.rc('xtick', labelsize='x-small')\n",
    "plt.rc('ytick', labelsize='x-small')\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(cost_history, ls='solid', color = 'black', label = '$\\gamma = 0.01$')\n",
    "ax.set_xlabel('epochs', fontsize = 16)\n",
    "ax.set_ylabel('Cost function $J$', fontsize = 16)\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., fontsize = 16)\n",
    "plt.tick_params(labelsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train = sess.run(accuracy, feed_dict={x: x_train, y: y_train})\n",
    "acc_test = sess.run(accuracy, feed_dict={x: x_test, y: y_test})\n",
    "print(\" accurary on the training set\", str.format('{0:.2f}', acc_train))\n",
    "print(\" accurary on the testing set\", str.format('{0:.2f}', acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

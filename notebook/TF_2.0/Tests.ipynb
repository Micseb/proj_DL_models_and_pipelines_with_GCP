{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Issue 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issue https://github.com/tensorflow/tensorflow/issues/27696"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0410 10:15:55.945059 4556207552 cross_device_ops.py:1111] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.\n",
      "I0410 10:15:55.945975 4556207552 run_config.py:532] Initializing RunConfig with distribution strategies.\n",
      "I0410 10:15:55.946563 4556207552 estimator_training.py:167] Not using Distribute Coordinator.\n",
      "W0410 10:15:55.948405 4556207552 estimator.py:1799] Using temporary folder as model directory: /var/folders/l7/00kxfwvs0vbbqxtrp3rpf3yh0000gn/T/tmpyvqcdnlw\n",
      "I0410 10:15:55.949310 4556207552 keras.py:464] Using the Keras model provided.\n",
      "I0410 10:15:56.822856 4556207552 estimator.py:202] Using config: {'_model_dir': '/var/folders/l7/00kxfwvs0vbbqxtrp3rpf3yh0000gn/T/tmpyvqcdnlw', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0xb2f9c0128>, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0xb2f9c0c18>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_distribute_coordinator_mode': None}\n",
      "I0410 10:15:56.824487 4556207552 estimator_training.py:186] Not using Distribute Coordinator.\n",
      "I0410 10:15:56.825150 4556207552 training.py:612] Running training and evaluation locally (non-distributed).\n",
      "I0410 10:15:56.826030 4556207552 training.py:700] Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
      "I0410 10:15:56.828684 4556207552 dataset_builder.py:157] Overwrite dataset info from restored data version.\n",
      "I0410 10:15:56.833184 4556207552 dataset_builder.py:193] Reusing dataset mnist (/Users/tarrade/tensorflow_datasets/mnist/1.0.0)\n",
      "I0410 10:15:57.163295 123145705418752 estimator.py:1126] Calling model_fn.\n",
      "I0410 10:15:57.164299 123145705418752 coordinator.py:219] Error reported to Coordinator: Only TensorFlow native optimizers are supported with DistributionStrategy.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tarrade/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\n",
      "    yield\n",
      "  File \"/Users/tarrade/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/distribute/mirrored_strategy.py\", line 882, in run\n",
      "    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\n",
      "  File \"/Users/tarrade/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1127, in _call_model_fn\n",
      "    model_fn_results = self._model_fn(features=features, **kwargs)\n",
      "  File \"/Users/tarrade/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/keras.py\", line 278, in model_fn\n",
      "    raise ValueError('Only TensorFlow native optimizers are supported with '\n",
      "ValueError: Only TensorFlow native optimizers are supported with DistributionStrategy.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Only TensorFlow native optimizers are supported with DistributionStrategy.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-490d351436b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m )\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(estimator, train_spec, eval_spec)\u001b[0m\n\u001b[1;32m    471\u001b[0m         '(with task id 0).  Given task id {}'.format(config.task_id))\n\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    611\u001b[0m         config.task_type != run_config_lib.TaskType.EVALUATOR):\n\u001b[1;32m    612\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Running training and evaluation locally (non-distributed).'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m     \u001b[0;31m# Distributed case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\u001b[0m in \u001b[0;36mrun_local\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m         \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_hooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m         saving_listeners=saving_listeners)\n\u001b[0m\u001b[1;32m    715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m     eval_result = listener_for_eval.eval_result or _EvalResult(\n",
      "\u001b[0;32m~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1135\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_distribution\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1137\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_distributed\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1198\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_distribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m       return self._actual_train_model_distributed(\n\u001b[0;32m-> 1200\u001b[0;31m           self._config._train_distribute, input_fn, hooks, saving_listeners)\n\u001b[0m\u001b[1;32m   1201\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_actual_train_model_distributed\u001b[0;34m(self, strategy, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1267\u001b[0m                     \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# although this will be None it seems\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m                     \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1269\u001b[0;31m                     self.config))\n\u001b[0m\u001b[1;32m   1270\u001b[0m           loss = strategy.reduce(reduce_util.ReduceOp.SUM,\n\u001b[1;32m   1271\u001b[0m                                  grouped_estimator_spec.loss)\n",
      "\u001b[0;32m~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1076\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1078\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/distribute/mirrored_strategy.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    663\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m     return _call_for_each_replica(self._container_strategy(), self._device_map,\n\u001b[0;32m--> 665\u001b[0;31m                                   fn, args, kwargs)\n\u001b[0m\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m   def _configure(self,\n",
      "\u001b[0;32m~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/distribute/mirrored_strategy.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[0;34m(distribution, device_map, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m       \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_run\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m     \u001b[0mcoord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthreads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_result\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, threads, stop_grace_period_secs, ignore_live_threads)\u001b[0m\n\u001b[1;32m    387\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_registered_threads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exc_info_to_raise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exc_info_to_raise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0mstragglers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mignore_live_threads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py\u001b[0m in \u001b[0;36mstop_on_exception\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \"\"\"\n\u001b[1;32m    296\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m       \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=bare-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/distribute/mirrored_strategy.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    880\u001b[0m               self._captured_var_scope, reuse=self.replica_id > 0), \\\n\u001b[1;32m    881\u001b[0m           \u001b[0mvariable_scope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_creator_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m   1125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1127\u001b[0;31m     \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1128\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/keras.py\u001b[0m in \u001b[0;36mmodel_fn\u001b[0;34m(features, labels, mode)\u001b[0m\n\u001b[1;32m    276\u001b[0m         not isinstance(keras_model.optimizer,\n\u001b[1;32m    277\u001b[0m                        (tf_optimizer_module.Optimizer, optimizers.TFOptimizer)):\n\u001b[0;32m--> 278\u001b[0;31m       raise ValueError('Only TensorFlow native optimizers are supported with '\n\u001b[0m\u001b[1;32m    279\u001b[0m                        'DistributionStrategy.')\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Only TensorFlow native optimizers are supported with DistributionStrategy."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from absl import logging\n",
    "\n",
    "logging.set_verbosity(logging.INFO)\n",
    "# Define the estimator's input_fn\n",
    "STEPS_PER_EPOCH = 5\n",
    "#BUFFER_SIZE = 10 # Use a much larger value for real code. \n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "\n",
    "def input_fn():\n",
    "    datasets, ds_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "    mnist_train, mnist_test = datasets['train'], datasets['test']\n",
    "\n",
    "    BUFFER_SIZE = 10000\n",
    "    BATCH_SIZE = 64\n",
    "\n",
    "    def scale(image, label):\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        image /= 255\n",
    "    \n",
    "        return image, label[..., tf.newaxis]\n",
    "\n",
    "    train_data = mnist_train.map(scale).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "    return train_data.repeat()\n",
    "\n",
    "# Define train & eval specs\n",
    "train_spec = tf.estimator.TrainSpec(input_fn=input_fn,\n",
    "                                    max_steps=STEPS_PER_EPOCH * NUM_EPOCHS)\n",
    "eval_spec = tf.estimator.EvalSpec(input_fn=input_fn,\n",
    "                                  steps=STEPS_PER_EPOCH)\n",
    "\n",
    "def make_model():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, 3, activation='relu',\n",
    "                               kernel_regularizer=tf.keras.regularizers.l2(0.02),\n",
    "                               input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "model = make_model()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#####\n",
    "## works\n",
    "#strategy=None \n",
    "## crash\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# config tf.estimator to use a give strategy\n",
    "training_config = tf.estimator.RunConfig(train_distribute=strategy)\n",
    "#####\n",
    "\n",
    "estimator = tf.keras.estimator.model_to_estimator(\n",
    "    keras_model = model,\n",
    "    config=training_config\n",
    ")\n",
    "\n",
    "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Issue 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issue https://github.com/tensorflow/tensorflow/issues/27581"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0410 13:06:58.001753 4570760640 dataset_builder.py:157] Overwrite dataset info from restored data version.\n",
      "I0410 13:06:58.004968 4570760640 dataset_builder.py:193] Reusing dataset mnist (/Users/tarrade/tensorflow_datasets/mnist/1.0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_27 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_27 (Flatten)         (None, 5408)              0         \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 5408)              0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 64)                346176    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_27 (B (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 347,402\n",
      "Trainable params: 347,274\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "None\n",
      "train\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 5s 981ms/step - loss: 1.5474 - accuracy: 0.5031\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.7533 - accuracy: 0.7875\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.6117 - accuracy: 0.8531\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 0.4893 - accuracy: 0.8750\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.4656 - accuracy: 0.8781\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.4816 - accuracy: 0.8875\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.4525 - accuracy: 0.8844\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 0.3243 - accuracy: 0.9125\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.4528 - accuracy: 0.8813\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.3727 - accuracy: 0.9094\n",
      "evaluate\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.3746 - accuracy: 0.8125\n",
      "predict on batch\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "dataset.make_initializable_iterator is not supported when eager execution is enabled.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmake_initializable_iterator\u001b[0;34m(dataset, shared_name)\u001b[0m\n\u001b[1;32m   1852\u001b[0m     \u001b[0;31m# some datasets (e.g. for prefetching) override its behavior.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_initializable_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshared_name\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RepeatDataset' object has no attribute '_make_initializable_iterator'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-8e6413a8aad2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"predict on batch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1359\u001b[0m     \u001b[0;31m# Validate and standardize user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     inputs, _, _ = self._standardize_user_data(\n\u001b[0;32m-> 1361\u001b[0;31m         x, extract_tensors_from_dataset=True)\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       if (isinstance(inputs, iterator_ops.EagerIterator) or\n",
      "\u001b[0;32m~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2439\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mextract_tensors_from_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2440\u001b[0m         \u001b[0;31m# We do this for `train_on_batch`/etc.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2441\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_tensors_from_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2442\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2443\u001b[0m       \u001b[0;31m# Graph mode iterator. We extract the symbolic tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mextract_tensors_from_dataset\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     \u001b[0mTuple\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0my\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mentry\u001b[0m \u001b[0mmay\u001b[0m \u001b[0mbe\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m   \"\"\"\n\u001b[0;32m-> 1384\u001b[0;31m   \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m   \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpack_iterator_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mget_iterator\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m   1362\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;34m\"\"\"Create and initialize an iterator from a dataset.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1364\u001b[0;31m   \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_initializable_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1365\u001b[0m   \u001b[0minitialize_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmake_initializable_iterator\u001b[0;34m(dataset, shared_name)\u001b[0m\n\u001b[1;32m   1853\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_initializable_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshared_name\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1854\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1855\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDatasetV1Adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_initializable_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshared_name\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_make_initializable_iterator\u001b[0;34m(self, shared_name)\u001b[0m\n\u001b[1;32m   1495\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1496\u001b[0m       raise RuntimeError(\n\u001b[0;32m-> 1497\u001b[0;31m           \u001b[0;34m\"dataset.make_initializable_iterator is not supported when eager \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1498\u001b[0m           \"execution is enabled.\")\n\u001b[1;32m   1499\u001b[0m     \u001b[0m_ensure_same_dataset_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: dataset.make_initializable_iterator is not supported when eager execution is enabled."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from absl import logging\n",
    "\n",
    "logging.set_verbosity(logging.INFO)\n",
    "# Define the estimator's input_fn\n",
    "STEPS_PER_EPOCH = 5\n",
    "#BUFFER_SIZE = 10 # Use a much larger value for real code. \n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "\n",
    "def input_fn():\n",
    "    datasets, ds_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "    mnist_train, mnist_test = datasets['train'], datasets['test']\n",
    "\n",
    "    BUFFER_SIZE = 10000\n",
    "    BATCH_SIZE = 64\n",
    "\n",
    "    def scale(image, label):\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        image /= 255\n",
    "    \n",
    "        return image, label[..., tf.newaxis]\n",
    "\n",
    "    train_data = mnist_train.map(scale).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "    return train_data.repeat()\n",
    "\n",
    "\n",
    "def make_model():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, 3, activation='relu',\n",
    "                               kernel_regularizer=tf.keras.regularizers.l2(0.02),\n",
    "                               input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "model = make_model()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "training_dataset=input_fn()\n",
    "\n",
    "print(\"train\")\n",
    "model.fit(training_dataset,\n",
    "          steps_per_epoch=5,\n",
    "          epochs=10,\n",
    "          verbose = 1)\n",
    "\n",
    "print(\"evaluate\")\n",
    "model.evaluate(training_dataset,\n",
    "              steps=1)\n",
    "\n",
    "print(\"predict on batch\")\n",
    "model.predict_on_batch(training_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Issue 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0410 16:05:10.988730 4409329088 keras.py:464] Using the Keras model provided.\n",
      "I0410 16:05:11.900668 4409329088 estimator.py:202] Using config: {'_model_dir': '/tmp/test2', '_tf_random_seed': None, '_save_summary_steps': 10, '_save_checkpoints_steps': 10, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 3, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0xb26349940>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "I0410 16:05:11.901903 4409329088 estimator_training.py:186] Not using Distribute Coordinator.\n",
      "I0410 16:05:11.902629 4409329088 training.py:612] Running training and evaluation locally (non-distributed).\n",
      "I0410 16:05:11.903352 4409329088 training.py:700] Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 10 or save_checkpoints_secs None.\n",
      "W0410 16:05:11.909771 4409329088 deprecation.py:323] From /Users/tarrade/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "I0410 16:05:11.922177 4409329088 dataset_builder.py:157] Overwrite dataset info from restored data version.\n",
      "I0410 16:05:11.926182 4409329088 dataset_builder.py:193] Reusing dataset mnist (/Users/tarrade/tensorflow_datasets/mnist/1.0.0)\n",
      "I0410 16:05:12.252431 4409329088 estimator.py:1126] Calling model_fn.\n",
      "I0410 16:05:12.720719 4409329088 estimator.py:1128] Done calling model_fn.\n",
      "I0410 16:05:12.721511 4409329088 estimator.py:1332] Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='/tmp/test2/keras/keras_model.ckpt', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\n",
      "I0410 16:05:12.722684 4409329088 warm_starting_util.py:417] Warm-starting from: ('/tmp/test2/keras/keras_model.ckpt',)\n",
      "I0410 16:05:12.723562 4409329088 warm_starting_util.py:466] Warm-starting variable: conv2d_1/kernel; prev_var_name: Unchanged\n",
      "I0410 16:05:12.724298 4409329088 warm_starting_util.py:466] Warm-starting variable: conv2d_1/bias; prev_var_name: Unchanged\n",
      "I0410 16:05:12.725471 4409329088 warm_starting_util.py:466] Warm-starting variable: dense_2/kernel; prev_var_name: Unchanged\n",
      "I0410 16:05:12.726055 4409329088 warm_starting_util.py:466] Warm-starting variable: dense_2/bias; prev_var_name: Unchanged\n",
      "I0410 16:05:12.726614 4409329088 warm_starting_util.py:466] Warm-starting variable: batch_normalization_v2_1/gamma; prev_var_name: Unchanged\n",
      "I0410 16:05:12.727260 4409329088 warm_starting_util.py:466] Warm-starting variable: batch_normalization_v2_1/beta; prev_var_name: Unchanged\n",
      "I0410 16:05:12.728409 4409329088 warm_starting_util.py:466] Warm-starting variable: dense_3/kernel; prev_var_name: Unchanged\n",
      "I0410 16:05:12.729074 4409329088 warm_starting_util.py:466] Warm-starting variable: dense_3/bias; prev_var_name: Unchanged\n",
      "I0410 16:05:12.786702 4409329088 basic_session_run_hooks.py:527] Create CheckpointSaverHook.\n",
      "I0410 16:05:13.122231 4409329088 monitored_session.py:241] Graph was finalized.\n",
      "I0410 16:05:13.276386 4409329088 session_manager.py:500] Running local_init_op.\n",
      "I0410 16:05:13.309736 4409329088 session_manager.py:502] Done running local_init_op.\n",
      "I0410 16:05:13.957747 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 0 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:15.566095 4409329088 basic_session_run_hooks.py:249] loss = 2.8199403, step = 0\n",
      "I0410 16:05:15.863330 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 10 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:15.976433 4409329088 dataset_builder.py:157] Overwrite dataset info from restored data version.\n",
      "I0410 16:05:15.979038 4409329088 dataset_builder.py:193] Reusing dataset mnist (/Users/tarrade/tensorflow_datasets/mnist/1.0.0)\n",
      "I0410 16:05:16.196389 4409329088 estimator.py:1126] Calling model_fn.\n",
      "I0410 16:05:16.351131 4409329088 estimator.py:1128] Done calling model_fn.\n",
      "W0410 16:05:16.352396 4409329088 deprecation.py:323] From /Users/tarrade/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py:363: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "I0410 16:05:16.387181 4409329088 evaluation.py:257] Starting evaluation at 2019-04-10T14:05:16Z\n",
      "I0410 16:05:16.504642 4409329088 monitored_session.py:241] Graph was finalized.\n",
      "W0410 16:05:16.505980 4409329088 deprecation.py:323] From /Users/tarrade/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "I0410 16:05:16.507546 4409329088 saver.py:1280] Restoring parameters from /tmp/test2/model.ckpt-10\n",
      "I0410 16:05:16.564421 4409329088 session_manager.py:500] Running local_init_op.\n",
      "I0410 16:05:16.584282 4409329088 session_manager.py:502] Done running local_init_op.\n",
      "I0410 16:05:17.839287 4409329088 evaluation.py:169] Evaluation [1/1]\n",
      "I0410 16:05:17.899185 4409329088 evaluation.py:277] Finished evaluation at 2019-04-10-14:05:17\n",
      "I0410 16:05:17.900545 4409329088 estimator.py:2027] Saving dict for global step 10: accuracy = 0.515625, global_step = 10, loss = 1.8932579\n",
      "I0410 16:05:17.992731 4409329088 estimator.py:2087] Saving 'checkpoint_path' summary for global step 10: /tmp/test2/model.ckpt-10\n",
      "I0410 16:05:18.331874 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 20 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:18.436731 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:18.623626 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 30 into /tmp/test2/model.ckpt.\n",
      "W0410 16:05:18.654033 4409329088 deprecation.py:323] From /Users/tarrade/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/training/saver.py:965: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "I0410 16:05:18.739454 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:18.929743 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 40 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:19.035972 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:19.224894 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 50 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:19.333423 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:19.359953 4409329088 basic_session_run_hooks.py:680] global_step/sec: 13.1782\n",
      "I0410 16:05:19.361182 4409329088 basic_session_run_hooks.py:247] loss = 0.28131837, step = 50 (3.795 sec)\n",
      "I0410 16:05:19.538797 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 60 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:19.649056 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:19.849131 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 70 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:19.951091 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0410 16:05:20.217498 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 80 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:20.335002 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:20.546483 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 90 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:20.672310 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:20.861371 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 100 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:20.965673 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:20.992710 4409329088 basic_session_run_hooks.py:680] global_step/sec: 30.6242\n",
      "I0410 16:05:20.994359 4409329088 basic_session_run_hooks.py:247] loss = 0.29835695, step = 100 (1.633 sec)\n",
      "I0410 16:05:21.164472 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 110 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:21.266270 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:21.466957 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 120 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:21.566915 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:21.739745 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 130 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:21.842414 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:22.037521 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 140 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:22.147839 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:22.338741 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 150 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:22.475365 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:22.500506 4409329088 basic_session_run_hooks.py:680] global_step/sec: 33.1592\n",
      "I0410 16:05:22.502062 4409329088 basic_session_run_hooks.py:247] loss = 0.20174369, step = 150 (1.508 sec)\n",
      "I0410 16:05:22.673966 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 160 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:22.778007 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:22.960743 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 170 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:23.066309 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:23.249605 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 180 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:23.350587 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:23.536225 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 190 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:23.648231 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:23.829802 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 200 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:23.941967 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:23.964138 4409329088 basic_session_run_hooks.py:680] global_step/sec: 34.1615\n",
      "I0410 16:05:23.965583 4409329088 basic_session_run_hooks.py:247] loss = 0.15657784, step = 200 (1.464 sec)\n",
      "I0410 16:05:24.135578 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 210 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:24.240700 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:24.502573 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 220 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:24.653499 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:24.992516 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 230 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:25.120429 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:25.301060 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 240 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:25.406584 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:25.617275 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 250 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:25.759738 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:25.789538 4409329088 basic_session_run_hooks.py:680] global_step/sec: 27.392\n",
      "I0410 16:05:25.791332 4409329088 basic_session_run_hooks.py:247] loss = 0.09616533, step = 250 (1.826 sec)\n",
      "I0410 16:05:25.970288 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 260 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:26.087126 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:26.294708 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 270 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:26.400227 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:26.599418 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 280 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:26.703485 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:26.893425 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 290 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:27.002274 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:27.192101 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 300 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:27.304383 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:27.360113 4409329088 basic_session_run_hooks.py:680] global_step/sec: 31.8348\n",
      "I0410 16:05:27.361684 4409329088 basic_session_run_hooks.py:247] loss = 0.13878596, step = 300 (1.570 sec)\n",
      "I0410 16:05:27.648148 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 310 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:27.764645 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:27.960125 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 320 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:28.065667 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:28.248077 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 330 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:28.371497 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:28.607066 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 340 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:28.711344 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:28.914588 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 350 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:29.029519 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:29.052209 4409329088 basic_session_run_hooks.py:680] global_step/sec: 29.5489\n",
      "I0410 16:05:29.053493 4409329088 basic_session_run_hooks.py:247] loss = 0.13861506, step = 350 (1.692 sec)\n",
      "I0410 16:05:29.236156 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 360 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:29.363868 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:29.604120 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 370 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:29.714661 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0410 16:05:29.919013 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 380 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:30.024296 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:30.200930 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 390 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:30.326547 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:30.545419 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 400 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:30.648465 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:30.672751 4409329088 basic_session_run_hooks.py:680] global_step/sec: 30.854\n",
      "I0410 16:05:30.674766 4409329088 basic_session_run_hooks.py:247] loss = 0.071289375, step = 400 (1.621 sec)\n",
      "I0410 16:05:30.846753 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 410 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:30.947962 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:31.137181 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 420 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:31.238681 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:31.434433 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 430 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:31.537238 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:31.721920 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 440 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:31.822579 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:32.008592 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 450 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:32.110789 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:32.132771 4409329088 basic_session_run_hooks.py:680] global_step/sec: 34.2459\n",
      "I0410 16:05:32.134170 4409329088 basic_session_run_hooks.py:247] loss = 0.090713225, step = 450 (1.459 sec)\n",
      "I0410 16:05:32.305234 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 460 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:32.407950 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:32.594221 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 470 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:32.696068 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:32.887469 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 480 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:32.995326 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:33.203054 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 490 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:33.306959 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:33.477289 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 500 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:33.578543 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:33.601577 4409329088 basic_session_run_hooks.py:680] global_step/sec: 34.0413\n",
      "I0410 16:05:33.602917 4409329088 basic_session_run_hooks.py:247] loss = 0.16824579, step = 500 (1.469 sec)\n",
      "I0410 16:05:33.781662 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 510 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:33.883854 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:34.066023 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 520 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:34.168881 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:34.346593 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 530 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:34.449068 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:34.642192 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 540 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:34.746112 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:34.929652 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 550 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:35.036274 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:35.059603 4409329088 basic_session_run_hooks.py:680] global_step/sec: 34.2931\n",
      "I0410 16:05:35.060995 4409329088 basic_session_run_hooks.py:247] loss = 0.11052929, step = 550 (1.458 sec)\n",
      "I0410 16:05:35.222176 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 560 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:35.326831 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:35.502810 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 570 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:35.611377 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:35.789323 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 580 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:35.890578 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:36.068346 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 590 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:36.171752 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:36.348718 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 600 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:36.452971 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:36.476567 4409329088 basic_session_run_hooks.py:680] global_step/sec: 35.2865\n",
      "I0410 16:05:36.477896 4409329088 basic_session_run_hooks.py:247] loss = 0.121245295, step = 600 (1.417 sec)\n",
      "I0410 16:05:36.636578 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 610 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:36.740186 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:36.921509 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 620 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:37.028337 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:37.204409 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 630 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:37.309039 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:37.483278 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 640 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:37.589027 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:37.766557 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 650 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:37.870750 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:37.897109 4409329088 basic_session_run_hooks.py:680] global_step/sec: 35.1981\n",
      "I0410 16:05:37.898971 4409329088 basic_session_run_hooks.py:247] loss = 0.07590674, step = 650 (1.421 sec)\n",
      "I0410 16:05:38.075665 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 660 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:38.179233 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:38.356288 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 670 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:38.462288 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0410 16:05:38.648912 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 680 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:38.754024 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:38.934735 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 690 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:39.050889 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:39.283418 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 700 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:39.389667 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:39.415289 4409329088 basic_session_run_hooks.py:680] global_step/sec: 32.934\n",
      "I0410 16:05:39.416693 4409329088 basic_session_run_hooks.py:247] loss = 0.06517317, step = 700 (1.518 sec)\n",
      "I0410 16:05:39.594142 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 710 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:39.697432 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:39.879647 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 720 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:39.987501 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:40.178403 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 730 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:40.283711 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:40.464128 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 740 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:40.570702 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:40.770883 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 750 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:40.883471 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:40.909804 4409329088 basic_session_run_hooks.py:680] global_step/sec: 33.4559\n",
      "I0410 16:05:40.911367 4409329088 basic_session_run_hooks.py:247] loss = 0.062433314, step = 750 (1.495 sec)\n",
      "I0410 16:05:41.097093 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 760 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:41.199522 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:41.378928 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 770 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:41.485131 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:41.706405 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 780 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:41.809616 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:41.930593 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 790 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:42.034816 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:42.149325 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 800 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:42.254626 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:42.270473 4409329088 basic_session_run_hooks.py:680] global_step/sec: 36.7459\n",
      "I0410 16:05:42.271813 4409329088 basic_session_run_hooks.py:247] loss = 0.073853984, step = 800 (1.360 sec)\n",
      "I0410 16:05:42.395511 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 810 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:42.499587 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:42.621986 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 820 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:42.734035 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:42.855978 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 830 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:42.962825 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:43.086253 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 840 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:43.193636 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:43.312922 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 850 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:43.420563 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:43.439880 4409329088 basic_session_run_hooks.py:680] global_step/sec: 42.7573\n",
      "I0410 16:05:43.441229 4409329088 basic_session_run_hooks.py:247] loss = 0.090155035, step = 850 (1.169 sec)\n",
      "I0410 16:05:43.569298 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 860 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:43.680611 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:43.801760 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 870 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:43.908930 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:44.035424 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 880 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:44.140573 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:44.262968 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 890 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:44.366904 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:44.481450 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 900 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:44.584439 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:44.601020 4409329088 basic_session_run_hooks.py:680] global_step/sec: 43.0613\n",
      "I0410 16:05:44.602233 4409329088 basic_session_run_hooks.py:247] loss = 0.09664778, step = 900 (1.161 sec)\n",
      "I0410 16:05:44.708693 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 910 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:44.815677 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:44.938998 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 920 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:45.039915 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:45.167378 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 930 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:45.272459 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:46.541370 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 940 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:46.647826 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:46.829080 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 950 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:47.035341 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:47.059824 4409329088 basic_session_run_hooks.py:680] global_step/sec: 20.3352\n",
      "I0410 16:05:47.061206 4409329088 basic_session_run_hooks.py:247] loss = 0.1252703, step = 950 (2.459 sec)\n",
      "I0410 16:05:47.216037 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 960 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:47.320943 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:47.511195 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 970 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:47.613228 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0410 16:05:47.797791 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 980 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:47.904359 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:48.099824 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 990 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:48.222608 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:48.459000 4409329088 basic_session_run_hooks.py:594] Saving checkpoints for 1000 into /tmp/test2/model.ckpt.\n",
      "I0410 16:05:48.567751 4409329088 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0410 16:05:48.577336 4409329088 dataset_builder.py:157] Overwrite dataset info from restored data version.\n",
      "I0410 16:05:48.580050 4409329088 dataset_builder.py:193] Reusing dataset mnist (/Users/tarrade/tensorflow_datasets/mnist/1.0.0)\n",
      "I0410 16:05:48.786550 4409329088 estimator.py:1126] Calling model_fn.\n",
      "I0410 16:05:48.933313 4409329088 estimator.py:1128] Done calling model_fn.\n",
      "I0410 16:05:48.958732 4409329088 evaluation.py:257] Starting evaluation at 2019-04-10T14:05:48Z\n",
      "I0410 16:05:49.048498 4409329088 monitored_session.py:241] Graph was finalized.\n",
      "I0410 16:05:49.050616 4409329088 saver.py:1280] Restoring parameters from /tmp/test2/model.ckpt-1000\n",
      "I0410 16:05:49.101088 4409329088 session_manager.py:500] Running local_init_op.\n",
      "I0410 16:05:49.117341 4409329088 session_manager.py:502] Done running local_init_op.\n",
      "I0410 16:05:50.464389 4409329088 evaluation.py:169] Evaluation [1/1]\n",
      "I0410 16:05:50.523463 4409329088 evaluation.py:277] Finished evaluation at 2019-04-10-14:05:50\n",
      "I0410 16:05:50.524246 4409329088 estimator.py:2027] Saving dict for global step 1000: accuracy = 1.0, global_step = 1000, loss = 0.03654488\n",
      "I0410 16:05:50.525436 4409329088 estimator.py:2087] Saving 'checkpoint_path' summary for global step 1000: /tmp/test2/model.ckpt-1000\n",
      "I0410 16:05:50.597305 4409329088 estimator.py:360] Loss for final step: 0.10749032.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train the model to estimator\n",
      "let inspect events.out.tfevents.* files\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from absl import logging\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import numpy as np\n",
    "\n",
    "logging.set_verbosity(logging.INFO)\n",
    "# Define the estimator's input_fn\n",
    "STEPS_PER_EPOCH = 5\n",
    "#BUFFER_SIZE = 10 # Use a much larger value for real code. \n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 1000\n",
    "\n",
    "\n",
    "def input_fn():\n",
    "    datasets, ds_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "    mnist_train, mnist_test = datasets['train'], datasets['test']\n",
    "\n",
    "    BUFFER_SIZE = 10000\n",
    "    BATCH_SIZE = 64\n",
    "\n",
    "    def scale(image, label):\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        image /= 255\n",
    "    \n",
    "        return image, label[..., tf.newaxis]\n",
    "\n",
    "    train_data = mnist_train.map(scale).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "    return train_data.repeat()\n",
    "\n",
    "def make_model():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, 3, activation='relu',\n",
    "                               kernel_regularizer=tf.keras.regularizers.l2(0.02),\n",
    "                               input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "model = make_model()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# config tf.estimator to use a give strategy\n",
    "training_config = tf.estimator.RunConfig(model_dir='/tmp/test2',\n",
    "                                         save_summary_steps=10,  # save summary every n steps\n",
    "                                         save_checkpoints_steps=10,  # save model every iteration (needed for eval)\n",
    "                                         # save_checkpoints_secs=10,\n",
    "                                         keep_checkpoint_max=3,  # keep last n models\n",
    "                                         log_step_count_steps=50)\n",
    "\n",
    "# Define train & eval specs\n",
    "train_spec = tf.estimator.TrainSpec(input_fn=input_fn,\n",
    "                                    max_steps=1000)\n",
    "eval_spec = tf.estimator.EvalSpec(input_fn=input_fn,\n",
    "                                  steps=1)\n",
    "\n",
    "estimator = tf.keras.estimator.model_to_estimator(\n",
    "    keras_model = model,\n",
    "    config=training_config\n",
    ")\n",
    "\n",
    "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
    "\n",
    "print('train the model to estimator')\n",
    "#estimator.train(input_fn=input_fn,\n",
    "#               steps=1000)\n",
    "\n",
    "def load_data_tensorboard(path):\n",
    "    event_acc = event_accumulator.EventAccumulator(path)\n",
    "    event_acc.Reload()\n",
    "    data = {}\n",
    "\n",
    "    for tag in sorted(event_acc.Tags()[\"scalars\"]):\n",
    "        x, y = [], []\n",
    "        for scalar_event in event_acc.Scalars(tag):\n",
    "            x.append(scalar_event.step)\n",
    "            y.append(scalar_event.value)\n",
    "        data[tag] = (np.asarray(x), np.asarray(y))\n",
    "    return data\n",
    "\n",
    "print('let inspect events.out.tfevents.* files')\n",
    "#print(' ')\n",
    "#print('training:')\n",
    "#history_train=load_data_tensorboard('/tmp/test2/')\n",
    "#print(history_train.keys())\n",
    "#print(history_train['loss_1'])\n",
    "#print(' ')\n",
    "#print('evaluation:')\n",
    "#history_eval=load_data_tensorboard('/tmp/test2/eval')\n",
    "#print(history_eval.keys())\n",
    "#print(history_eval['loss'][1])\n",
    "#print(history_eval['accuracy'][1])\n",
    "#print()\n",
    "#print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 28488\r\n",
      "drwxr-xr-x  16 tarrade  wheel      512 Apr 10 16:05 \u001b[34m.\u001b[m\u001b[m\r\n",
      "drwxrwxrwt  24 root     wheel      768 Apr 10 16:05 \u001b[30m\u001b[42m..\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 tarrade  wheel      177 Apr 10 16:05 checkpoint\r\n",
      "drwxr-xr-x   3 tarrade  wheel       96 Apr 10 16:05 \u001b[34meval\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 tarrade  wheel   814064 Apr 10 16:05 events.out.tfevents.1554905112.Fabien-Tarrades-MacBook-Pro.local\r\n",
      "-rw-r--r--   1 tarrade  wheel   542088 Apr 10 16:05 graph.pbtxt\r\n",
      "drwxr-xr-x   6 tarrade  wheel      192 Apr 10 16:05 \u001b[34mkeras\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 tarrade  wheel  4167828 Apr 10 16:05 model.ckpt-1000.data-00000-of-00001\r\n",
      "-rw-r--r--   1 tarrade  wheel     1168 Apr 10 16:05 model.ckpt-1000.index\r\n",
      "-rw-r--r--   1 tarrade  wheel   230862 Apr 10 16:05 model.ckpt-1000.meta\r\n",
      "-rw-r--r--   1 tarrade  wheel  4167828 Apr 10 16:05 model.ckpt-980.data-00000-of-00001\r\n",
      "-rw-r--r--   1 tarrade  wheel     1168 Apr 10 16:05 model.ckpt-980.index\r\n",
      "-rw-r--r--   1 tarrade  wheel   230862 Apr 10 16:05 model.ckpt-980.meta\r\n",
      "-rw-r--r--   1 tarrade  wheel  4167828 Apr 10 16:05 model.ckpt-990.data-00000-of-00001\r\n",
      "-rw-r--r--   1 tarrade  wheel     1168 Apr 10 16:05 model.ckpt-990.index\r\n",
      "-rw-r--r--   1 tarrade  wheel   230862 Apr 10 16:05 model.ckpt-990.meta\r\n"
     ]
    }
   ],
   "source": [
    "!ls -la /tmp/test2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard.notebook extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard.notebook\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6013 (pid 84571), started -1 day, 23:21:03 ago. (Use '!kill 84571' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:6013\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x104245898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard.notebook\n",
    "%tensorboard  --logdir   {'/tmp/test2'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Issue 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0410 22:03:37.601018 4631340480 dataset_builder.py:157] Overwrite dataset info from restored data version.\n",
      "I0410 22:03:37.605882 4631340480 dataset_builder.py:193] Reusing dataset mnist (/Users/tarrade/tensorflow_datasets/mnist/1.0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 5408)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 5408)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                346176    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 347,402\n",
      "Trainable params: 347,274\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "None\n",
      "train\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 3s 503ms/step - loss: 1.6584 - accuracy: 0.5031\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.7385 - accuracy: 0.7875\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.5663 - accuracy: 0.8406\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4718 - accuracy: 0.9000\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.4651 - accuracy: 0.8906\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.4008 - accuracy: 0.9062\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.3242 - accuracy: 0.9312\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 50ms/step - loss: 0.4088 - accuracy: 0.8906\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.3412 - accuracy: 0.9187\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 53ms/step - loss: 0.3471 - accuracy: 0.9250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb30e1f240>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from absl import logging\n",
    "\n",
    "logging.set_verbosity(logging.INFO)\n",
    "# Define the estimator's input_fn\n",
    "STEPS_PER_EPOCH = 5\n",
    "#BUFFER_SIZE = 10 # Use a much larger value for real code. \n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "\n",
    "def input_fn():\n",
    "    datasets, ds_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "    mnist_train, mnist_test = datasets['train'], datasets['test']\n",
    "\n",
    "    BUFFER_SIZE = 10000\n",
    "    BATCH_SIZE = 64\n",
    "\n",
    "    def scale(image, label):\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        image /= 255\n",
    "    \n",
    "        return image, label[..., tf.newaxis]\n",
    "\n",
    "    train_data = mnist_train.map(scale).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "    return train_data.repeat()\n",
    "\n",
    "\n",
    "def make_model():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, 3, activation='relu',\n",
    "                               kernel_regularizer=tf.keras.regularizers.l2(0.02),\n",
    "                               input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "tbCallBack=tf.keras.callbacks.TensorBoard(log_dir='/tmp/test3/')\n",
    "                                          #histogram_freq=1,\n",
    "                                          #write_graph=True)\n",
    "\n",
    "model = make_model()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "training_dataset=input_fn()\n",
    "\n",
    "print(\"train\")\n",
    "model.fit(training_dataset,\n",
    "          steps_per_epoch=5,\n",
    "          epochs=10,\n",
    "          callbacks=[tbCallBack])\n",
    "\n",
    "#print(\"evaluate\")\n",
    "#model.evaluate(training_dataset,\n",
    "#              steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6015 (pid 86358), started -1 day, 23:09:10 ago. (Use '!kill 86358' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:6015\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x10ee7c2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard.notebook\n",
    "%tensorboard  --logdir   {'/tmp/test3'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\r\n",
      "drwxr-xr-x   5 tarrade  wheel  160 Apr 10 22:03 \u001b[34m.\u001b[m\u001b[m\r\n",
      "drwxrwxrwt  12 root     wheel  384 Apr 10 22:03 \u001b[30m\u001b[42m..\u001b[m\u001b[m\r\n",
      "drwxr-xr-x   3 tarrade  wheel   96 Apr 10 22:03 \u001b[34mplugins\u001b[m\u001b[m\r\n",
      "drwxr-xr-x   3 tarrade  wheel   96 Apr 10 22:03 \u001b[34mtrain\u001b[m\u001b[m\r\n",
      "drwxr-xr-x   3 tarrade  wheel   96 Apr 10 22:03 \u001b[34mvalidation\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls -la /tmp/test3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 48664\r\n",
      "drwxr-xr-x  14 tarrade  staff      448 Apr 10 21:17 \u001b[34m.\u001b[m\u001b[m\r\n",
      "drwxr-xr-x   4 tarrade  staff      128 Apr 10 21:13 \u001b[34m..\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 tarrade  staff      177 Apr 10 21:17 checkpoint\r\n",
      "-rw-r--r--   1 tarrade  staff   333317 Apr 10 21:13 graph.pbtxt\r\n",
      "drwxr-xr-x   6 tarrade  staff      192 Apr 10 21:13 \u001b[34mkeras\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 tarrade  staff  8036500 Apr 10 21:17 model.ckpt-1000.data-00000-of-00001\r\n",
      "-rw-r--r--   1 tarrade  staff      889 Apr 10 21:17 model.ckpt-1000.index\r\n",
      "-rw-r--r--   1 tarrade  staff   143942 Apr 10 21:17 model.ckpt-1000.meta\r\n",
      "-rw-r--r--   1 tarrade  staff  8036500 Apr 10 21:17 model.ckpt-980.data-00000-of-00001\r\n",
      "-rw-r--r--   1 tarrade  staff      889 Apr 10 21:17 model.ckpt-980.index\r\n",
      "-rw-r--r--   1 tarrade  staff   143942 Apr 10 21:17 model.ckpt-980.meta\r\n",
      "-rw-r--r--   1 tarrade  staff  8036500 Apr 10 21:17 model.ckpt-990.data-00000-of-00001\r\n",
      "-rw-r--r--   1 tarrade  staff      889 Apr 10 21:17 model.ckpt-990.index\r\n",
      "-rw-r--r--   1 tarrade  staff   143942 Apr 10 21:17 model.ckpt-990.meta\r\n"
     ]
    }
   ],
   "source": [
    "!ls -la ../../results/Models/Mnist/tf_1_12/estimator/v2/ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0408 14:08:27.861193 4771767744 cross_device_ops.py:1111] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.distribute.mirrored_strategy.MirroredStrategy at 0x113427eb8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0408 14:18:37.378367 4771767744 <ipython-input-8-2fd3015b66b4>:4] test\n",
      "W0408 14:18:37.384752 4771767744 cross_device_ops.py:1111] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.distribute.mirrored_strategy.MirroredStrategy at 0xb34700cf8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.DEBUG)\n",
    "logging.debug('test')\n",
    "tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.24.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# need to be defined if working behind a proxy\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS']\n",
    "os.environ['HTTPS_PROXY']\n",
    "os.environ['REQUESTS_CA_BUNDLE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['HTTPS_PROXY'] = \"https://proxy-url:Port\"\n",
    "#os.environ['REQUESTS_CA_BUNDLE'] = \"C:/Users/path/to/certs\"\n",
    "#os.environ[\"PROJECT_ID\"] = \"project-id-34914\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "project=os.environ['PROJECT_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\envs\\mnist\\lib\\site-packages\\google\\auth\\_default.py:66: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "client = storage.Client(project=project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in client.list_buckets():\n",
    "    print(b.name)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://googleapis.github.io/google-cloud-python/latest/bigquery/usage/pandas.html\n",
    "https://googleapis.github.io/google-cloud-python/latest/bigquery/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "client = bigquery.Client(project=project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row((2009, 343139, 99.7), {'Year': 0, 'Number_of_Questions': 1, 'Percent_Questions_with_Answers': 2})\n",
      "Row((2010, 693332, 99.1), {'Year': 0, 'Number_of_Questions': 1, 'Percent_Questions_with_Answers': 2})\n",
      "Row((2011, 1198587, 97.2), {'Year': 0, 'Number_of_Questions': 1, 'Percent_Questions_with_Answers': 2})\n",
      "Row((2012, 1642687, 94.6), {'Year': 0, 'Number_of_Questions': 1, 'Percent_Questions_with_Answers': 2})\n",
      "Row((2013, 2056613, 91.6), {'Year': 0, 'Number_of_Questions': 1, 'Percent_Questions_with_Answers': 2})\n",
      "Row((2014, 2160361, 88.5), {'Year': 0, 'Number_of_Questions': 1, 'Percent_Questions_with_Answers': 2})\n",
      "Row((2015, 2214389, 86.4), {'Year': 0, 'Number_of_Questions': 1, 'Percent_Questions_with_Answers': 2})\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"SELECT\n",
    "  EXTRACT(YEAR FROM creation_date) AS Year,\n",
    "  COUNT(*) AS Number_of_Questions,\n",
    "  ROUND(100 * SUM(IF(answer_count > 0, 1, 0)) / COUNT(*), 1) AS Percent_Questions_with_Answers\n",
    "FROM\n",
    "  `bigquery-public-data.stackoverflow.posts_questions`\n",
    "GROUP BY\n",
    "  Year\n",
    "HAVING\n",
    "  Year > 2008 AND Year < 2016\n",
    "ORDER BY\n",
    "  Year\n",
    "\"\"\"\n",
    "#df = client.query(query).to_dataframe()\n",
    "#df.head()\n",
    "query_job = client.query(query)\n",
    "rows = query_job.result()  # Waits for query to finish\n",
    "\n",
    "for row in rows:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys:  ('Year', 'Number_of_Questions', 'Percent_Questions_with_Answers')\n",
      "Values:  (2015, 2214389, 86.4)\n"
     ]
    }
   ],
   "source": [
    "print(\"Keys: \", tuple(row.keys()))\n",
    "print(\"Values: \", row.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Number_of_Questions</th>\n",
       "      <th>Percent_Questions_with_Answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009</td>\n",
       "      <td>343139</td>\n",
       "      <td>99.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010</td>\n",
       "      <td>693332</td>\n",
       "      <td>99.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011</td>\n",
       "      <td>1198587</td>\n",
       "      <td>97.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012</td>\n",
       "      <td>1642687</td>\n",
       "      <td>94.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013</td>\n",
       "      <td>2056613</td>\n",
       "      <td>91.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2014</td>\n",
       "      <td>2160361</td>\n",
       "      <td>88.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2015</td>\n",
       "      <td>2214389</td>\n",
       "      <td>86.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  Number_of_Questions  Percent_Questions_with_Answers\n",
       "0  2009               343139                            99.7\n",
       "1  2010               693332                            99.1\n",
       "2  2011              1198587                            97.2\n",
       "3  2012              1642687                            94.6\n",
       "4  2013              2056613                            91.6\n",
       "5  2014              2160361                            88.5\n",
       "6  2015              2214389                            86.4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = client.query(query).to_dataframe()\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:env_gcp_dl_2_0_alpha]",
   "language": "python",
   "name": "conda-env-env_gcp_dl_2_0_alpha-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

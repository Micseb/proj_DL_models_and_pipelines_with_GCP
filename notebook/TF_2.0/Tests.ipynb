{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Issue 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issue https://github.com/tensorflow/tensorflow/issues/27696"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0410 10:15:55.945059 4556207552 cross_device_ops.py:1111] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.\n",
      "I0410 10:15:55.945975 4556207552 run_config.py:532] Initializing RunConfig with distribution strategies.\n",
      "I0410 10:15:55.946563 4556207552 estimator_training.py:167] Not using Distribute Coordinator.\n",
      "W0410 10:15:55.948405 4556207552 estimator.py:1799] Using temporary folder as model directory: /var/folders/l7/00kxfwvs0vbbqxtrp3rpf3yh0000gn/T/tmpyvqcdnlw\n",
      "I0410 10:15:55.949310 4556207552 keras.py:464] Using the Keras model provided.\n",
      "I0410 10:15:56.822856 4556207552 estimator.py:202] Using config: {'_model_dir': '/var/folders/l7/00kxfwvs0vbbqxtrp3rpf3yh0000gn/T/tmpyvqcdnlw', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0xb2f9c0128>, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0xb2f9c0c18>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_distribute_coordinator_mode': None}\n",
      "I0410 10:15:56.824487 4556207552 estimator_training.py:186] Not using Distribute Coordinator.\n",
      "I0410 10:15:56.825150 4556207552 training.py:612] Running training and evaluation locally (non-distributed).\n",
      "I0410 10:15:56.826030 4556207552 training.py:700] Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
      "I0410 10:15:56.828684 4556207552 dataset_builder.py:157] Overwrite dataset info from restored data version.\n",
      "I0410 10:15:56.833184 4556207552 dataset_builder.py:193] Reusing dataset mnist (/Users/tarrade/tensorflow_datasets/mnist/1.0.0)\n",
      "I0410 10:15:57.163295 123145705418752 estimator.py:1126] Calling model_fn.\n",
      "I0410 10:15:57.164299 123145705418752 coordinator.py:219] Error reported to Coordinator: Only TensorFlow native optimizers are supported with DistributionStrategy.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tarrade/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\n",
      "    yield\n",
      "  File \"/Users/tarrade/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/distribute/mirrored_strategy.py\", line 882, in run\n",
      "    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\n",
      "  File \"/Users/tarrade/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1127, in _call_model_fn\n",
      "    model_fn_results = self._model_fn(features=features, **kwargs)\n",
      "  File \"/Users/tarrade/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/keras.py\", line 278, in model_fn\n",
      "    raise ValueError('Only TensorFlow native optimizers are supported with '\n",
      "ValueError: Only TensorFlow native optimizers are supported with DistributionStrategy.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Only TensorFlow native optimizers are supported with DistributionStrategy.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-490d351436b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m )\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(estimator, train_spec, eval_spec)\u001b[0m\n\u001b[1;32m    471\u001b[0m         '(with task id 0).  Given task id {}'.format(config.task_id))\n\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    611\u001b[0m         config.task_type != run_config_lib.TaskType.EVALUATOR):\n\u001b[1;32m    612\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Running training and evaluation locally (non-distributed).'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m     \u001b[0;31m# Distributed case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\u001b[0m in \u001b[0;36mrun_local\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m         \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_hooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m         saving_listeners=saving_listeners)\n\u001b[0m\u001b[1;32m    715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m     eval_result = listener_for_eval.eval_result or _EvalResult(\n",
      "\u001b[0;32m~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1135\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_distribution\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1137\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_distributed\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1198\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_distribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m       return self._actual_train_model_distributed(\n\u001b[0;32m-> 1200\u001b[0;31m           self._config._train_distribute, input_fn, hooks, saving_listeners)\n\u001b[0m\u001b[1;32m   1201\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_actual_train_model_distributed\u001b[0;34m(self, strategy, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1267\u001b[0m                     \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# although this will be None it seems\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m                     \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1269\u001b[0;31m                     self.config))\n\u001b[0m\u001b[1;32m   1270\u001b[0m           loss = strategy.reduce(reduce_util.ReduceOp.SUM,\n\u001b[1;32m   1271\u001b[0m                                  grouped_estimator_spec.loss)\n",
      "\u001b[0;32m~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1076\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1078\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/distribute/mirrored_strategy.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    663\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m     return _call_for_each_replica(self._container_strategy(), self._device_map,\n\u001b[0;32m--> 665\u001b[0;31m                                   fn, args, kwargs)\n\u001b[0m\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m   def _configure(self,\n",
      "\u001b[0;32m~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/distribute/mirrored_strategy.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[0;34m(distribution, device_map, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m       \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_run\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m     \u001b[0mcoord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthreads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_result\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, threads, stop_grace_period_secs, ignore_live_threads)\u001b[0m\n\u001b[1;32m    387\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_registered_threads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exc_info_to_raise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exc_info_to_raise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0mstragglers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mignore_live_threads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py\u001b[0m in \u001b[0;36mstop_on_exception\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \"\"\"\n\u001b[1;32m    296\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m       \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=bare-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/distribute/mirrored_strategy.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    880\u001b[0m               self._captured_var_scope, reuse=self.replica_id > 0), \\\n\u001b[1;32m    881\u001b[0m           \u001b[0mvariable_scope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_creator_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m   1125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1127\u001b[0;31m     \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1128\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/keras.py\u001b[0m in \u001b[0;36mmodel_fn\u001b[0;34m(features, labels, mode)\u001b[0m\n\u001b[1;32m    276\u001b[0m         not isinstance(keras_model.optimizer,\n\u001b[1;32m    277\u001b[0m                        (tf_optimizer_module.Optimizer, optimizers.TFOptimizer)):\n\u001b[0;32m--> 278\u001b[0;31m       raise ValueError('Only TensorFlow native optimizers are supported with '\n\u001b[0m\u001b[1;32m    279\u001b[0m                        'DistributionStrategy.')\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Only TensorFlow native optimizers are supported with DistributionStrategy."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from absl import logging\n",
    "\n",
    "logging.set_verbosity(logging.INFO)\n",
    "# Define the estimator's input_fn\n",
    "STEPS_PER_EPOCH = 5\n",
    "#BUFFER_SIZE = 10 # Use a much larger value for real code. \n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "\n",
    "def input_fn():\n",
    "    datasets, ds_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "    mnist_train, mnist_test = datasets['train'], datasets['test']\n",
    "\n",
    "    BUFFER_SIZE = 10000\n",
    "    BATCH_SIZE = 64\n",
    "\n",
    "    def scale(image, label):\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        image /= 255\n",
    "    \n",
    "        return image, label[..., tf.newaxis]\n",
    "\n",
    "    train_data = mnist_train.map(scale).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "    return train_data.repeat()\n",
    "\n",
    "# Define train & eval specs\n",
    "train_spec = tf.estimator.TrainSpec(input_fn=input_fn,\n",
    "                                    max_steps=STEPS_PER_EPOCH * NUM_EPOCHS)\n",
    "eval_spec = tf.estimator.EvalSpec(input_fn=input_fn,\n",
    "                                  steps=STEPS_PER_EPOCH)\n",
    "\n",
    "def make_model():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, 3, activation='relu',\n",
    "                               kernel_regularizer=tf.keras.regularizers.l2(0.02),\n",
    "                               input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "model = make_model()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#####\n",
    "## works\n",
    "#strategy=None \n",
    "## crash\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# config tf.estimator to use a give strategy\n",
    "training_config = tf.estimator.RunConfig(train_distribute=strategy)\n",
    "#####\n",
    "\n",
    "estimator = tf.keras.estimator.model_to_estimator(\n",
    "    keras_model = model,\n",
    "    config=training_config\n",
    ")\n",
    "\n",
    "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Issue 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issue https://github.com/tensorflow/tensorflow/issues/27581"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0417 11:31:40.511445 4620772800 dataset_builder.py:157] Overwrite dataset info from restored data version.\n",
      "I0417 11:31:40.516194 4620772800 dataset_builder.py:193] Reusing dataset mnist (/Users/tarrade/tensorflow_datasets/mnist/1.0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 5408)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 5408)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                346176    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 347,402\n",
      "Trainable params: 347,274\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "None\n",
      "train\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 2s 469ms/step - loss: 1.6783 - accuracy: 0.5031\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.7350 - accuracy: 0.8250\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.6950 - accuracy: 0.8125\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.5760 - accuracy: 0.8469\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.5196 - accuracy: 0.8750\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.5436 - accuracy: 0.8438\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.3721 - accuracy: 0.9062\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.4201 - accuracy: 0.9094\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3667 - accuracy: 0.9281\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.4475 - accuracy: 0.8813\n",
      "evaluate\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.2144 - accuracy: 0.8281\n",
      "predict on batch\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "dataset.make_initializable_iterator is not supported when eager execution is enabled.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmake_initializable_iterator\u001b[0;34m(dataset, shared_name)\u001b[0m\n\u001b[1;32m   1852\u001b[0m     \u001b[0;31m# some datasets (e.g. for prefetching) override its behavior.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_initializable_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshared_name\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RepeatDataset' object has no attribute '_make_initializable_iterator'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8e6413a8aad2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"predict on batch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1359\u001b[0m     \u001b[0;31m# Validate and standardize user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     inputs, _, _ = self._standardize_user_data(\n\u001b[0;32m-> 1361\u001b[0;31m         x, extract_tensors_from_dataset=True)\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       if (isinstance(inputs, iterator_ops.EagerIterator) or\n",
      "\u001b[0;32m~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2439\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mextract_tensors_from_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2440\u001b[0m         \u001b[0;31m# We do this for `train_on_batch`/etc.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2441\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_tensors_from_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2442\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2443\u001b[0m       \u001b[0;31m# Graph mode iterator. We extract the symbolic tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mextract_tensors_from_dataset\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     \u001b[0mTuple\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0my\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mentry\u001b[0m \u001b[0mmay\u001b[0m \u001b[0mbe\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m   \"\"\"\n\u001b[0;32m-> 1384\u001b[0;31m   \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m   \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpack_iterator_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mget_iterator\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m   1362\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;34m\"\"\"Create and initialize an iterator from a dataset.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1364\u001b[0;31m   \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_initializable_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1365\u001b[0m   \u001b[0minitialize_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmake_initializable_iterator\u001b[0;34m(dataset, shared_name)\u001b[0m\n\u001b[1;32m   1853\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_initializable_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshared_name\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1854\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1855\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDatasetV1Adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_initializable_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshared_name\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_make_initializable_iterator\u001b[0;34m(self, shared_name)\u001b[0m\n\u001b[1;32m   1495\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1496\u001b[0m       raise RuntimeError(\n\u001b[0;32m-> 1497\u001b[0;31m           \u001b[0;34m\"dataset.make_initializable_iterator is not supported when eager \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1498\u001b[0m           \"execution is enabled.\")\n\u001b[1;32m   1499\u001b[0m     \u001b[0m_ensure_same_dataset_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: dataset.make_initializable_iterator is not supported when eager execution is enabled."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from absl import logging\n",
    "\n",
    "logging.set_verbosity(logging.INFO)\n",
    "# Define the estimator's input_fn\n",
    "STEPS_PER_EPOCH = 5\n",
    "#BUFFER_SIZE = 10 # Use a much larger value for real code. \n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "\n",
    "def input_fn():\n",
    "    datasets, ds_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "    mnist_train, mnist_test = datasets['train'], datasets['test']\n",
    "\n",
    "    BUFFER_SIZE = 10000\n",
    "    BATCH_SIZE = 64\n",
    "\n",
    "    def scale(image, label):\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        image /= 255\n",
    "    \n",
    "        return image, label[..., tf.newaxis]\n",
    "\n",
    "    train_data = mnist_train.map(scale).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "    return train_data.repeat()\n",
    "\n",
    "\n",
    "def make_model():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, 3, activation='relu',\n",
    "                               kernel_regularizer=tf.keras.regularizers.l2(0.02),\n",
    "                               input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "model = make_model()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "training_dataset=input_fn()\n",
    "\n",
    "print(\"train\")\n",
    "model.fit(training_dataset,\n",
    "          steps_per_epoch=5,\n",
    "          epochs=10,\n",
    "          verbose = 1)\n",
    "\n",
    "print(\"evaluate\")\n",
    "model.evaluate(training_dataset,\n",
    "              steps=1)\n",
    "\n",
    "print(\"predict on batch\")\n",
    "model.predict_on_batch(training_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Issue 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0418 10:32:48.354762 4620772800 keras.py:464] Using the Keras model provided.\n",
      "I0418 10:32:49.495244 4620772800 estimator.py:202] Using config: {'_model_dir': '/tmp/test2', '_tf_random_seed': None, '_save_summary_steps': 10, '_save_checkpoints_steps': 10, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 3, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0xb33e095c0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "I0418 10:32:49.496958 4620772800 estimator_training.py:186] Not using Distribute Coordinator.\n",
      "I0418 10:32:49.498065 4620772800 training.py:612] Running training and evaluation locally (non-distributed).\n",
      "I0418 10:32:49.499068 4620772800 training.py:700] Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 10 or save_checkpoints_secs None.\n",
      "W0418 10:32:49.511196 4620772800 deprecation.py:323] From /Users/tarrade/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "I0418 10:32:49.528229 4620772800 dataset_builder.py:157] Overwrite dataset info from restored data version.\n",
      "I0418 10:32:49.538066 4620772800 dataset_builder.py:193] Reusing dataset mnist (/Users/tarrade/tensorflow_datasets/mnist/1.0.0)\n",
      "I0418 10:32:49.813451 4620772800 estimator.py:1126] Calling model_fn.\n",
      "I0418 10:32:50.311661 4620772800 estimator.py:1128] Done calling model_fn.\n",
      "I0418 10:32:50.312506 4620772800 estimator.py:1332] Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='/tmp/test2/keras/keras_model.ckpt', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\n",
      "I0418 10:32:50.314095 4620772800 warm_starting_util.py:417] Warm-starting from: ('/tmp/test2/keras/keras_model.ckpt',)\n",
      "I0418 10:32:50.315121 4620772800 warm_starting_util.py:466] Warm-starting variable: conv2d_6/kernel; prev_var_name: Unchanged\n",
      "I0418 10:32:50.316829 4620772800 warm_starting_util.py:466] Warm-starting variable: conv2d_6/bias; prev_var_name: Unchanged\n",
      "I0418 10:32:50.317548 4620772800 warm_starting_util.py:466] Warm-starting variable: dense_15/kernel; prev_var_name: Unchanged\n",
      "I0418 10:32:50.318291 4620772800 warm_starting_util.py:466] Warm-starting variable: dense_15/bias; prev_var_name: Unchanged\n",
      "I0418 10:32:50.320221 4620772800 warm_starting_util.py:466] Warm-starting variable: batch_normalization_v2_6/gamma; prev_var_name: Unchanged\n",
      "I0418 10:32:50.321673 4620772800 warm_starting_util.py:466] Warm-starting variable: batch_normalization_v2_6/beta; prev_var_name: Unchanged\n",
      "I0418 10:32:50.322259 4620772800 warm_starting_util.py:466] Warm-starting variable: dense_16/kernel; prev_var_name: Unchanged\n",
      "I0418 10:32:50.323199 4620772800 warm_starting_util.py:466] Warm-starting variable: dense_16/bias; prev_var_name: Unchanged\n",
      "I0418 10:32:50.371983 4620772800 basic_session_run_hooks.py:527] Create CheckpointSaverHook.\n",
      "I0418 10:32:51.695770 4620772800 monitored_session.py:241] Graph was finalized.\n",
      "I0418 10:32:51.838419 4620772800 session_manager.py:500] Running local_init_op.\n",
      "I0418 10:32:51.861543 4620772800 session_manager.py:502] Done running local_init_op.\n",
      "I0418 10:32:52.346680 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 0 into /tmp/test2/model.ckpt.\n",
      "I0418 10:32:54.548731 4620772800 basic_session_run_hooks.py:249] loss = 2.349371, step = 0\n",
      "I0418 10:32:54.907222 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 10 into /tmp/test2/model.ckpt.\n",
      "I0418 10:32:55.027948 4620772800 dataset_builder.py:157] Overwrite dataset info from restored data version.\n",
      "I0418 10:32:55.030761 4620772800 dataset_builder.py:193] Reusing dataset mnist (/Users/tarrade/tensorflow_datasets/mnist/1.0.0)\n",
      "I0418 10:32:55.268581 4620772800 estimator.py:1126] Calling model_fn.\n",
      "I0418 10:32:55.435056 4620772800 estimator.py:1128] Done calling model_fn.\n",
      "W0418 10:32:55.435945 4620772800 deprecation.py:323] From /Users/tarrade/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py:363: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "I0418 10:32:55.461799 4620772800 evaluation.py:257] Starting evaluation at 2019-04-18T08:32:55Z\n",
      "I0418 10:32:55.561755 4620772800 monitored_session.py:241] Graph was finalized.\n",
      "W0418 10:32:55.562728 4620772800 deprecation.py:323] From /Users/tarrade/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "I0418 10:32:55.564997 4620772800 saver.py:1280] Restoring parameters from /tmp/test2/model.ckpt-10\n",
      "I0418 10:32:55.636868 4620772800 session_manager.py:500] Running local_init_op.\n",
      "I0418 10:32:55.655051 4620772800 session_manager.py:502] Done running local_init_op.\n",
      "I0418 10:32:58.027405 4620772800 evaluation.py:169] Evaluation [1/1]\n",
      "I0418 10:32:58.151069 4620772800 evaluation.py:277] Finished evaluation at 2019-04-18-08:32:58\n",
      "I0418 10:32:58.151998 4620772800 estimator.py:2027] Saving dict for global step 10: accuracy = 0.390625, global_step = 10, loss = 1.8653859\n",
      "I0418 10:32:58.231112 4620772800 estimator.py:2087] Saving 'checkpoint_path' summary for global step 10: /tmp/test2/model.ckpt-10\n",
      "I0418 10:32:58.629379 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 20 into /tmp/test2/model.ckpt.\n",
      "I0418 10:32:58.749837 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:32:59.004991 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 30 into /tmp/test2/model.ckpt.\n",
      "W0418 10:32:59.039159 4620772800 deprecation.py:323] From /Users/tarrade/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/training/saver.py:965: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "I0418 10:32:59.115866 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:32:59.403847 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 40 into /tmp/test2/model.ckpt.\n",
      "I0418 10:32:59.507658 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:32:59.785468 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 50 into /tmp/test2/model.ckpt.\n",
      "I0418 10:32:59.890742 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:32:59.925611 4620772800 basic_session_run_hooks.py:680] global_step/sec: 9.29813\n",
      "I0418 10:32:59.926988 4620772800 basic_session_run_hooks.py:247] loss = 0.33484957, step = 50 (5.379 sec)\n",
      "I0418 10:33:00.256650 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 60 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:00.422080 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:00.719848 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 70 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:00.850507 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0418 10:33:01.104421 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 80 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:01.210136 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:01.460709 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 90 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:01.568709 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:01.823717 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 100 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:01.928065 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:01.957066 4620772800 basic_session_run_hooks.py:680] global_step/sec: 24.6102\n",
      "I0418 10:33:01.958796 4620772800 basic_session_run_hooks.py:247] loss = 0.24310794, step = 100 (2.032 sec)\n",
      "I0418 10:33:02.176247 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 110 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:02.295032 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:02.556681 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 120 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:02.662420 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:02.908045 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 130 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:03.029323 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:03.328076 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 140 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:03.428182 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:03.678032 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 150 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:03.795965 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:03.828178 4620772800 basic_session_run_hooks.py:680] global_step/sec: 26.722\n",
      "I0418 10:33:03.829594 4620772800 basic_session_run_hooks.py:247] loss = 0.120124884, step = 150 (1.871 sec)\n",
      "I0418 10:33:04.065304 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 160 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:04.170063 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:04.425679 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 170 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:04.552959 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:04.830640 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 180 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:04.962188 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:05.225564 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 190 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:05.343650 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:05.626303 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 200 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:05.756187 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:05.788016 4620772800 basic_session_run_hooks.py:680] global_step/sec: 25.5125\n",
      "I0418 10:33:05.789803 4620772800 basic_session_run_hooks.py:247] loss = 0.14241384, step = 200 (1.960 sec)\n",
      "I0418 10:33:06.019646 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 210 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:06.126917 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:06.390826 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 220 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:06.510157 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:06.768101 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 230 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:06.881052 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:07.150568 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 240 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:07.258354 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:07.516829 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 250 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:07.627767 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:07.658214 4620772800 basic_session_run_hooks.py:680] global_step/sec: 26.7344\n",
      "I0418 10:33:07.659543 4620772800 basic_session_run_hooks.py:247] loss = 0.20917657, step = 250 (1.870 sec)\n",
      "I0418 10:33:07.885823 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 260 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:08.005867 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:08.287439 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 270 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:08.389946 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:08.673961 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 280 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:08.857791 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:09.131343 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 290 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:09.239216 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:09.507612 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 300 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:09.620757 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:09.648070 4620772800 basic_session_run_hooks.py:680] global_step/sec: 25.1274\n",
      "I0418 10:33:09.649433 4620772800 basic_session_run_hooks.py:247] loss = 0.15272026, step = 300 (1.990 sec)\n",
      "I0418 10:33:09.878602 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 310 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:10.003465 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:10.296922 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 320 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:10.415606 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:10.682124 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 330 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:10.806604 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:11.096060 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 340 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:11.199532 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:11.465460 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 350 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:11.575376 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:11.607556 4620772800 basic_session_run_hooks.py:680] global_step/sec: 25.5176\n",
      "I0418 10:33:11.609528 4620772800 basic_session_run_hooks.py:247] loss = 0.21866627, step = 350 (1.960 sec)\n",
      "I0418 10:33:11.879143 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 360 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:12.004250 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:12.297260 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 370 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:12.404690 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0418 10:33:12.673862 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 380 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:12.791600 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:13.058331 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 390 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:13.173775 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:13.429002 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 400 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:13.545475 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:13.573698 4620772800 basic_session_run_hooks.py:680] global_step/sec: 25.4304\n",
      "I0418 10:33:13.575514 4620772800 basic_session_run_hooks.py:247] loss = 0.14879647, step = 400 (1.966 sec)\n",
      "I0418 10:33:13.822032 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 410 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:13.942348 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:14.191438 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 420 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:14.294521 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:14.536976 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 430 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:14.643577 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:14.912770 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 440 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:15.033097 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:15.294371 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 450 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:15.397856 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:15.421840 4620772800 basic_session_run_hooks.py:680] global_step/sec: 27.0536\n",
      "I0418 10:33:15.423324 4620772800 basic_session_run_hooks.py:247] loss = 0.09038646, step = 450 (1.848 sec)\n",
      "I0418 10:33:15.683659 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 460 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:15.798371 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:16.073621 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 470 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:16.192346 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:16.451071 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 480 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:16.580353 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:16.839703 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 490 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:16.952808 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:17.208321 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 500 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:17.316149 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:17.340728 4620772800 basic_session_run_hooks.py:680] global_step/sec: 26.0568\n",
      "I0418 10:33:17.342080 4620772800 basic_session_run_hooks.py:247] loss = 0.1384346, step = 500 (1.919 sec)\n",
      "I0418 10:33:17.550526 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 510 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:17.662145 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:17.931773 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 520 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:18.051621 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:18.346035 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 530 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:18.450630 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:18.710988 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 540 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:18.835514 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:19.124074 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 550 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:19.226950 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:19.255150 4620772800 basic_session_run_hooks.py:680] global_step/sec: 26.1176\n",
      "I0418 10:33:19.256857 4620772800 basic_session_run_hooks.py:247] loss = 0.15664607, step = 550 (1.915 sec)\n",
      "I0418 10:33:19.508450 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 560 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:19.655634 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:19.972773 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 570 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:20.089469 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:20.347739 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 580 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:20.455698 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:20.713285 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 590 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:20.859412 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:21.142982 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 600 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:21.276485 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:21.307771 4620772800 basic_session_run_hooks.py:680] global_step/sec: 24.359\n",
      "I0418 10:33:21.309777 4620772800 basic_session_run_hooks.py:247] loss = 0.14086144, step = 600 (2.053 sec)\n",
      "I0418 10:33:21.568042 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 610 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:21.679983 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:21.968967 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 620 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:22.088171 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:22.349944 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 630 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:22.458191 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:22.726128 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 640 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:22.837701 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:23.096076 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 650 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:23.208009 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:23.238553 4620772800 basic_session_run_hooks.py:680] global_step/sec: 25.8975\n",
      "I0418 10:33:23.240121 4620772800 basic_session_run_hooks.py:247] loss = 0.18968578, step = 650 (1.930 sec)\n",
      "I0418 10:33:23.483451 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 660 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:23.599471 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:23.860762 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 670 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:23.964061 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0418 10:33:24.244512 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 680 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:24.355572 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:24.636538 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 690 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:24.755194 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:25.015833 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 700 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:25.124302 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:25.154242 4620772800 basic_session_run_hooks.py:680] global_step/sec: 26.099\n",
      "I0418 10:33:25.155678 4620772800 basic_session_run_hooks.py:247] loss = 0.08401941, step = 700 (1.916 sec)\n",
      "I0418 10:33:25.400709 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 710 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:25.515573 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:25.778778 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 720 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:25.888825 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:26.143355 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 730 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:26.249834 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:26.516273 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 740 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:26.629332 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:26.919230 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 750 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:27.029549 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:27.054030 4620772800 basic_session_run_hooks.py:680] global_step/sec: 26.3191\n",
      "I0418 10:33:27.055433 4620772800 basic_session_run_hooks.py:247] loss = 0.12723206, step = 750 (1.900 sec)\n",
      "I0418 10:33:27.295534 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 760 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:27.407771 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:27.687957 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 770 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:27.813072 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:28.120283 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 780 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:28.232026 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:28.402683 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 790 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:28.513000 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:28.655123 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 800 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:28.762826 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:28.780472 4620772800 basic_session_run_hooks.py:680] global_step/sec: 28.9608\n",
      "I0418 10:33:28.781873 4620772800 basic_session_run_hooks.py:247] loss = 0.083507925, step = 800 (1.726 sec)\n",
      "I0418 10:33:28.906520 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 810 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:29.016304 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:29.161432 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 820 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:29.278392 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:29.427253 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 830 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:29.542602 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:29.689873 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 840 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:29.801284 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:29.939821 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 850 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:30.060432 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:30.080169 4620772800 basic_session_run_hooks.py:680] global_step/sec: 38.471\n",
      "I0418 10:33:30.081717 4620772800 basic_session_run_hooks.py:247] loss = 0.059149753, step = 850 (1.300 sec)\n",
      "I0418 10:33:30.222184 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 860 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:30.343150 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:30.500072 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 870 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:30.613766 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:30.753021 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 880 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:30.858541 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:30.994446 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 890 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:31.103507 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:31.242162 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 900 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:31.347850 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:31.366133 4620772800 basic_session_run_hooks.py:680] global_step/sec: 38.8809\n",
      "I0418 10:33:31.367316 4620772800 basic_session_run_hooks.py:247] loss = 0.083088934, step = 900 (1.286 sec)\n",
      "I0418 10:33:31.487298 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 910 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:31.605823 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:31.740000 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 920 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:31.848619 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:31.982753 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 930 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:32.109872 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:34.358509 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 940 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:34.469349 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:34.759767 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 950 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:34.870749 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:34.905572 4620772800 basic_session_run_hooks.py:680] global_step/sec: 14.1269\n",
      "I0418 10:33:34.907201 4620772800 basic_session_run_hooks.py:247] loss = 0.11215803, step = 950 (3.540 sec)\n",
      "I0418 10:33:35.166193 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 960 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:35.274659 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:35.552072 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 970 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:35.670428 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0418 10:33:35.949110 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 980 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:36.058755 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:36.332733 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 990 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:36.440008 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:36.748717 4620772800 basic_session_run_hooks.py:594] Saving checkpoints for 1000 into /tmp/test2/model.ckpt.\n",
      "I0418 10:33:36.857003 4620772800 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
      "I0418 10:33:36.867471 4620772800 dataset_builder.py:157] Overwrite dataset info from restored data version.\n",
      "I0418 10:33:36.870707 4620772800 dataset_builder.py:193] Reusing dataset mnist (/Users/tarrade/tensorflow_datasets/mnist/1.0.0)\n",
      "I0418 10:33:37.105168 4620772800 estimator.py:1126] Calling model_fn.\n",
      "I0418 10:33:37.258127 4620772800 estimator.py:1128] Done calling model_fn.\n",
      "I0418 10:33:37.283377 4620772800 evaluation.py:257] Starting evaluation at 2019-04-18T08:33:37Z\n",
      "I0418 10:33:37.368888 4620772800 monitored_session.py:241] Graph was finalized.\n",
      "I0418 10:33:37.371830 4620772800 saver.py:1280] Restoring parameters from /tmp/test2/model.ckpt-1000\n",
      "I0418 10:33:37.421555 4620772800 session_manager.py:500] Running local_init_op.\n",
      "I0418 10:33:37.437994 4620772800 session_manager.py:502] Done running local_init_op.\n",
      "I0418 10:33:39.654585 4620772800 evaluation.py:169] Evaluation [1/1]\n",
      "I0418 10:33:39.714818 4620772800 evaluation.py:277] Finished evaluation at 2019-04-18-08:33:39\n",
      "I0418 10:33:39.715597 4620772800 estimator.py:2027] Saving dict for global step 1000: accuracy = 1.0, global_step = 1000, loss = 0.04869295\n",
      "I0418 10:33:39.717011 4620772800 estimator.py:2087] Saving 'checkpoint_path' summary for global step 1000: /tmp/test2/model.ckpt-1000\n",
      "I0418 10:33:39.785672 4620772800 estimator.py:360] Loss for final step: 0.15195937.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train the model to estimator\n",
      "let inspect events.out.tfevents.* files\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from absl import logging\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import numpy as np\n",
    "\n",
    "logging.set_verbosity(logging.INFO)\n",
    "# Define the estimator's input_fn\n",
    "STEPS_PER_EPOCH = 5\n",
    "#BUFFER_SIZE = 10 # Use a much larger value for real code. \n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 1000\n",
    "\n",
    "\n",
    "def input_fn():\n",
    "    datasets, ds_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "    mnist_train, mnist_test = datasets['train'], datasets['test']\n",
    "\n",
    "    BUFFER_SIZE = 10000\n",
    "    BATCH_SIZE = 64\n",
    "\n",
    "    def scale(image, label):\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        image /= 255\n",
    "    \n",
    "        return image, label[..., tf.newaxis]\n",
    "\n",
    "    train_data = mnist_train.map(scale).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "    return train_data.repeat()\n",
    "\n",
    "def make_model():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, 3, activation='relu',\n",
    "                               kernel_regularizer=tf.keras.regularizers.l2(0.02),\n",
    "                               input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "model = make_model()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# config tf.estimator to use a give strategy\n",
    "training_config = tf.estimator.RunConfig(model_dir='/tmp/test2',\n",
    "                                         save_summary_steps=10,  # save summary every n steps\n",
    "                                         save_checkpoints_steps=10,  # save model every iteration (needed for eval)\n",
    "                                         # save_checkpoints_secs=10,\n",
    "                                         keep_checkpoint_max=3,  # keep last n models\n",
    "                                         log_step_count_steps=50)\n",
    "\n",
    "# Define train & eval specs\n",
    "train_spec = tf.estimator.TrainSpec(input_fn=input_fn,\n",
    "                                    max_steps=1000)\n",
    "eval_spec = tf.estimator.EvalSpec(input_fn=input_fn,\n",
    "                                  steps=1)\n",
    "\n",
    "estimator = tf.keras.estimator.model_to_estimator(\n",
    "    keras_model = model,\n",
    "    config=training_config\n",
    ")\n",
    "\n",
    "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
    "\n",
    "print('train the model to estimator')\n",
    "#estimator.train(input_fn=input_fn,\n",
    "#               steps=1000)\n",
    "\n",
    "def load_data_tensorboard(path):\n",
    "    event_acc = event_accumulator.EventAccumulator(path)\n",
    "    event_acc.Reload()\n",
    "    data = {}\n",
    "\n",
    "    for tag in sorted(event_acc.Tags()[\"scalars\"]):\n",
    "        x, y = [], []\n",
    "        for scalar_event in event_acc.Scalars(tag):\n",
    "            x.append(scalar_event.step)\n",
    "            y.append(scalar_event.value)\n",
    "        data[tag] = (np.asarray(x), np.asarray(y))\n",
    "    return data\n",
    "\n",
    "print('let inspect events.out.tfevents.* files')\n",
    "#print(' ')\n",
    "#print('training:')\n",
    "#history_train=load_data_tensorboard('/tmp/test2/')\n",
    "#print(history_train.keys())\n",
    "#print(history_train['loss_1'])\n",
    "#print(' ')\n",
    "#print('evaluation:')\n",
    "#history_eval=load_data_tensorboard('/tmp/test2/eval')\n",
    "#print(history_eval.keys())\n",
    "#print(history_eval['loss'][1])\n",
    "#print(history_eval['accuracy'][1])\n",
    "#print()\n",
    "#print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 28488\r\n",
      "drwxr-xr-x  16 tarrade  wheel      512 Apr 10 16:05 \u001b[34m.\u001b[m\u001b[m\r\n",
      "drwxrwxrwt  24 root     wheel      768 Apr 10 16:05 \u001b[30m\u001b[42m..\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 tarrade  wheel      177 Apr 10 16:05 checkpoint\r\n",
      "drwxr-xr-x   3 tarrade  wheel       96 Apr 10 16:05 \u001b[34meval\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 tarrade  wheel   814064 Apr 10 16:05 events.out.tfevents.1554905112.Fabien-Tarrades-MacBook-Pro.local\r\n",
      "-rw-r--r--   1 tarrade  wheel   542088 Apr 10 16:05 graph.pbtxt\r\n",
      "drwxr-xr-x   6 tarrade  wheel      192 Apr 10 16:05 \u001b[34mkeras\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 tarrade  wheel  4167828 Apr 10 16:05 model.ckpt-1000.data-00000-of-00001\r\n",
      "-rw-r--r--   1 tarrade  wheel     1168 Apr 10 16:05 model.ckpt-1000.index\r\n",
      "-rw-r--r--   1 tarrade  wheel   230862 Apr 10 16:05 model.ckpt-1000.meta\r\n",
      "-rw-r--r--   1 tarrade  wheel  4167828 Apr 10 16:05 model.ckpt-980.data-00000-of-00001\r\n",
      "-rw-r--r--   1 tarrade  wheel     1168 Apr 10 16:05 model.ckpt-980.index\r\n",
      "-rw-r--r--   1 tarrade  wheel   230862 Apr 10 16:05 model.ckpt-980.meta\r\n",
      "-rw-r--r--   1 tarrade  wheel  4167828 Apr 10 16:05 model.ckpt-990.data-00000-of-00001\r\n",
      "-rw-r--r--   1 tarrade  wheel     1168 Apr 10 16:05 model.ckpt-990.index\r\n",
      "-rw-r--r--   1 tarrade  wheel   230862 Apr 10 16:05 model.ckpt-990.meta\r\n"
     ]
    }
   ],
   "source": [
    "!ls -la /tmp/test2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard.notebook extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard.notebook\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6013 (pid 84571), started -1 day, 23:21:03 ago. (Use '!kill 84571' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:6013\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x104245898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard.notebook\n",
    "%tensorboard  --logdir   {'/tmp/test2'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Issue 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0417 13:41:14.449864 4620772800 dataset_builder.py:157] Overwrite dataset info from restored data version.\n",
      "I0417 13:41:14.452956 4620772800 dataset_builder.py:193] Reusing dataset mnist (/Users/tarrade/tensorflow_datasets/mnist/1.0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 5408)              0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 5408)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                346176    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_5 (Ba (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 347,402\n",
      "Trainable params: 347,274\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "None\n",
      "train\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 3s 603ms/step - loss: 1.4433 - accuracy: 0.5781\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.7295 - accuracy: 0.8062\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.5221 - accuracy: 0.8656\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.4369 - accuracy: 0.8844\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.4093 - accuracy: 0.8813\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.2764 - accuracy: 0.9438\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3123 - accuracy: 0.9187\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.2823 - accuracy: 0.9312\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.2870 - accuracy: 0.9031\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.2306 - accuracy: 0.9312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb2dbfc438>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from absl import logging\n",
    "\n",
    "logging.set_verbosity(logging.INFO)\n",
    "# Define the estimator's input_fn\n",
    "STEPS_PER_EPOCH = 5\n",
    "#BUFFER_SIZE = 10 # Use a much larger value for real code. \n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "def input_fn():\n",
    "    datasets, ds_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "    mnist_train, mnist_test = datasets['train'], datasets['test']\n",
    "\n",
    "    BUFFER_SIZE = 10000\n",
    "    BATCH_SIZE = 64\n",
    "\n",
    "    def scale(image, label):\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        image /= 255\n",
    "    \n",
    "        return image, label[..., tf.newaxis]\n",
    "\n",
    "    train_data = mnist_train.map(scale).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "    return train_data.repeat()\n",
    "\n",
    "\n",
    "def make_model():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, 3, activation='relu',\n",
    "                               kernel_regularizer=tf.keras.regularizers.l2(0.02),\n",
    "                               input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "tbCallBack=tf.keras.callbacks.TensorBoard(log_dir='/tmp/test3/',\n",
    "                                          histogram_freq=1,\n",
    "                                          write_graph=True)\n",
    "\n",
    "model = make_model()\n",
    "\n",
    "optimiser=tf.keras.optimizers.Adam(lr=0.01, beta_1=0.9, epsilon=1e-07)\n",
    "\n",
    "model.compile(optimizer=optimiser,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "training_dataset=input_fn()\n",
    "\n",
    "print(\"train\")\n",
    "model.fit(training_dataset,\n",
    "          steps_per_epoch=5,\n",
    "          epochs=10,\n",
    "          callbacks=[tbCallBack],\n",
    "          verbose =1)\n",
    "\n",
    "#print(\"evaluate\")\n",
    "#model.evaluate(training_dataset,\n",
    "#              steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard.notebook extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard.notebook\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 17713), started -1 day, 23:03:03 ago. (Use '!kill 17713' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:6006\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x10a8cbc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard.notebook\n",
    "%tensorboard  --logdir   {'/tmp/test3'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\r\n",
      "drwxr-xr-x   5 tarrade  wheel  160 Apr 10 22:03 \u001b[34m.\u001b[m\u001b[m\r\n",
      "drwxrwxrwt  12 root     wheel  384 Apr 10 22:03 \u001b[30m\u001b[42m..\u001b[m\u001b[m\r\n",
      "drwxr-xr-x   3 tarrade  wheel   96 Apr 10 22:03 \u001b[34mplugins\u001b[m\u001b[m\r\n",
      "drwxr-xr-x   3 tarrade  wheel   96 Apr 10 22:03 \u001b[34mtrain\u001b[m\u001b[m\r\n",
      "drwxr-xr-x   3 tarrade  wheel   96 Apr 10 22:03 \u001b[34mvalidation\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls -la /tmp/test3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 48664\r\n",
      "drwxr-xr-x  14 tarrade  staff      448 Apr 10 21:17 \u001b[34m.\u001b[m\u001b[m\r\n",
      "drwxr-xr-x   4 tarrade  staff      128 Apr 10 21:13 \u001b[34m..\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 tarrade  staff      177 Apr 10 21:17 checkpoint\r\n",
      "-rw-r--r--   1 tarrade  staff   333317 Apr 10 21:13 graph.pbtxt\r\n",
      "drwxr-xr-x   6 tarrade  staff      192 Apr 10 21:13 \u001b[34mkeras\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 tarrade  staff  8036500 Apr 10 21:17 model.ckpt-1000.data-00000-of-00001\r\n",
      "-rw-r--r--   1 tarrade  staff      889 Apr 10 21:17 model.ckpt-1000.index\r\n",
      "-rw-r--r--   1 tarrade  staff   143942 Apr 10 21:17 model.ckpt-1000.meta\r\n",
      "-rw-r--r--   1 tarrade  staff  8036500 Apr 10 21:17 model.ckpt-980.data-00000-of-00001\r\n",
      "-rw-r--r--   1 tarrade  staff      889 Apr 10 21:17 model.ckpt-980.index\r\n",
      "-rw-r--r--   1 tarrade  staff   143942 Apr 10 21:17 model.ckpt-980.meta\r\n",
      "-rw-r--r--   1 tarrade  staff  8036500 Apr 10 21:17 model.ckpt-990.data-00000-of-00001\r\n",
      "-rw-r--r--   1 tarrade  staff      889 Apr 10 21:17 model.ckpt-990.index\r\n",
      "-rw-r--r--   1 tarrade  staff   143942 Apr 10 21:17 model.ckpt-990.meta\r\n"
     ]
    }
   ],
   "source": [
    "!ls -la ../../results/Models/Mnist/tf_1_12/estimator/v2/ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0408 14:08:27.861193 4771767744 cross_device_ops.py:1111] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.distribute.mirrored_strategy.MirroredStrategy at 0x113427eb8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0408 14:18:37.378367 4771767744 <ipython-input-8-2fd3015b66b4>:4] test\n",
      "W0408 14:18:37.384752 4771767744 cross_device_ops.py:1111] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.distribute.mirrored_strategy.MirroredStrategy at 0xb34700cf8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.DEBUG)\n",
    "logging.debug('test')\n",
    "tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.24.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# need to be defined if working behind a proxy\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS']\n",
    "os.environ['HTTPS_PROXY']\n",
    "os.environ['REQUESTS_CA_BUNDLE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['HTTPS_PROXY'] = \"https://proxy-url:Port\"\n",
    "#os.environ['REQUESTS_CA_BUNDLE'] = \"C:/Users/path/to/certs\"\n",
    "#os.environ[\"PROJECT_ID\"] = \"project-id-34914\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "project=os.environ['PROJECT_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\envs\\mnist\\lib\\site-packages\\google\\auth\\_default.py:66: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "client = storage.Client(project=project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in client.list_buckets():\n",
    "    print(b.name)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://googleapis.github.io/google-cloud-python/latest/bigquery/usage/pandas.html\n",
    "https://googleapis.github.io/google-cloud-python/latest/bigquery/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "client = bigquery.Client(project=project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row((2009, 343139, 99.7), {'Year': 0, 'Number_of_Questions': 1, 'Percent_Questions_with_Answers': 2})\n",
      "Row((2010, 693332, 99.1), {'Year': 0, 'Number_of_Questions': 1, 'Percent_Questions_with_Answers': 2})\n",
      "Row((2011, 1198587, 97.2), {'Year': 0, 'Number_of_Questions': 1, 'Percent_Questions_with_Answers': 2})\n",
      "Row((2012, 1642687, 94.6), {'Year': 0, 'Number_of_Questions': 1, 'Percent_Questions_with_Answers': 2})\n",
      "Row((2013, 2056613, 91.6), {'Year': 0, 'Number_of_Questions': 1, 'Percent_Questions_with_Answers': 2})\n",
      "Row((2014, 2160361, 88.5), {'Year': 0, 'Number_of_Questions': 1, 'Percent_Questions_with_Answers': 2})\n",
      "Row((2015, 2214389, 86.4), {'Year': 0, 'Number_of_Questions': 1, 'Percent_Questions_with_Answers': 2})\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"SELECT\n",
    "  EXTRACT(YEAR FROM creation_date) AS Year,\n",
    "  COUNT(*) AS Number_of_Questions,\n",
    "  ROUND(100 * SUM(IF(answer_count > 0, 1, 0)) / COUNT(*), 1) AS Percent_Questions_with_Answers\n",
    "FROM\n",
    "  `bigquery-public-data.stackoverflow.posts_questions`\n",
    "GROUP BY\n",
    "  Year\n",
    "HAVING\n",
    "  Year > 2008 AND Year < 2016\n",
    "ORDER BY\n",
    "  Year\n",
    "\"\"\"\n",
    "#df = client.query(query).to_dataframe()\n",
    "#df.head()\n",
    "query_job = client.query(query)\n",
    "rows = query_job.result()  # Waits for query to finish\n",
    "\n",
    "for row in rows:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys:  ('Year', 'Number_of_Questions', 'Percent_Questions_with_Answers')\n",
      "Values:  (2015, 2214389, 86.4)\n"
     ]
    }
   ],
   "source": [
    "print(\"Keys: \", tuple(row.keys()))\n",
    "print(\"Values: \", row.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Number_of_Questions</th>\n",
       "      <th>Percent_Questions_with_Answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009</td>\n",
       "      <td>343139</td>\n",
       "      <td>99.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010</td>\n",
       "      <td>693332</td>\n",
       "      <td>99.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011</td>\n",
       "      <td>1198587</td>\n",
       "      <td>97.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012</td>\n",
       "      <td>1642687</td>\n",
       "      <td>94.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013</td>\n",
       "      <td>2056613</td>\n",
       "      <td>91.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2014</td>\n",
       "      <td>2160361</td>\n",
       "      <td>88.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2015</td>\n",
       "      <td>2214389</td>\n",
       "      <td>86.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  Number_of_Questions  Percent_Questions_with_Answers\n",
       "0  2009               343139                            99.7\n",
       "1  2010               693332                            99.1\n",
       "2  2011              1198587                            97.2\n",
       "3  2012              1642687                            94.6\n",
       "4  2013              2056613                            91.6\n",
       "5  2014              2160361                            88.5\n",
       "6  2015              2214389                            86.4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = client.query(query).to_dataframe()\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:env_gcp_dl_2_0_alpha]",
   "language": "python",
   "name": "conda-env-env_gcp_dl_2_0_alpha-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
